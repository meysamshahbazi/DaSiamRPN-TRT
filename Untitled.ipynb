{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2765e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import time\n",
    "import math\n",
    "# import torch.nn.functional as F\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764caf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# DaSiamRPN\n",
    "# Licensed under The MIT License\n",
    "# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n",
    "# --------------------------------------------------------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "size = 1\n",
    "configs = [3, 96, 256, 384, 384, 256]\n",
    "configs = list(map(lambda x: 3 if x==3 else x*size, configs))\n",
    "feat_in = configs[-1]\n",
    "feature_out = 256\n",
    "anchor=5\n",
    "class Temple(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(Temple, self).__init__()\n",
    "        self.cfg = {'lr': 0.30, 'window_influence': 0.40, 'penalty_k': 0.22, 'instance_size': 271, 'adaptive': True} # 0.655\n",
    "\n",
    "        self.featureExtract = nn.Sequential(\n",
    "            nn.Conv2d(configs[0], configs[1] , kernel_size=11, stride=2),\n",
    "            nn.BatchNorm2d(configs[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[1], configs[2], kernel_size=5),\n",
    "            nn.BatchNorm2d(configs[2]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[2], configs[3], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[3]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[3], configs[4], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[4]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[4], configs[5], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[5]),\n",
    "        )\n",
    "\n",
    "        self.anchor = anchor\n",
    "        self.feature_out = feature_out\n",
    "\n",
    "        self.conv_r1 = nn.Conv2d(feat_in, feature_out*4*anchor, 3)\n",
    "        self.conv_r2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.conv_cls1 = nn.Conv2d(feat_in, feature_out*2*anchor, 3)\n",
    "        self.conv_cls2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.regress_adjust = nn.Conv2d(4*anchor, 4*anchor, 1)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, z):\n",
    "        with torch.no_grad():\n",
    "            z_f = self.featureExtract(z)\n",
    "            r1_kernel_raw = self.conv_r1(z_f)\n",
    "            cls1_kernel_raw = self.conv_cls1(z_f)\n",
    "            kernel_size = r1_kernel_raw.data.size()[-1]\n",
    "            r1_kernel = r1_kernel_raw.view(self.anchor*4, self.feature_out, kernel_size, kernel_size)\n",
    "            cls1_kernel = cls1_kernel_raw.view(self.anchor*2, self.feature_out, kernel_size, kernel_size)\n",
    "        return r1_kernel,cls1_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65b6ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Temple()\n",
    "model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "# z = torch.randn(1,3,127,127).cuda()\n",
    "z = torch.randint(255,(1,3,127,127)).float().cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "ONNX_FILE_PATH = \"temple.onnx\"\n",
    "\n",
    "# torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"zf\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"r1_kernel\",\"cls1_kernel\"], export_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81c2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = 1\n",
    "configs = [3, 96, 256, 384, 384, 256]\n",
    "configs = list(map(lambda x: 3 if x==3 else x*size, configs))\n",
    "feat_in = configs[-1]\n",
    "feature_out = 256\n",
    "anchor=5\n",
    "class SiamRPN(nn.Module):\n",
    "    def __init__(self):\n",
    "#     def __init__(self, size=2, feature_out=512, anchor=5):\n",
    "        \n",
    "        super(SiamRPN, self).__init__()\n",
    "        self.featureExtract = nn.Sequential(\n",
    "            nn.Conv2d(configs[0], configs[1] , kernel_size=11, stride=2),\n",
    "            nn.BatchNorm2d(configs[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[1], configs[2], kernel_size=5),\n",
    "            nn.BatchNorm2d(configs[2]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[2], configs[3], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[3]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[3], configs[4], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[4]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[4], configs[5], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[5]),\n",
    "        )\n",
    "\n",
    "        self.anchor = anchor\n",
    "        self.feature_out = feature_out\n",
    "\n",
    "        self.conv_r1 = nn.Conv2d(feat_in, feature_out*4*anchor, 3)\n",
    "        self.conv_r2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.conv_cls1 = nn.Conv2d(feat_in, feature_out*2*anchor, 3)\n",
    "        self.conv_cls2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.regress_adjust = nn.Conv2d(4*anchor, 4*anchor, 1)\n",
    "\n",
    "#         self.r1_kernel = []\n",
    "#         self.cls1_kernel = []\n",
    "\n",
    "        self.cfg = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_f = self.featureExtract(x)\n",
    "        return self.conv_r2(x_f),self.conv_cls2(x_f)\n",
    "#         return self.regress_adjust(F.conv2d(self.conv_r2(x_f), self.r1_kernel)), \\\n",
    "#                F.conv2d(self.conv_cls2(x_f), self.cls1_kernel)\n",
    "    \n",
    "        \n",
    "#     def temple(self, z):\n",
    "#         z_f = self.featureExtract(z)\n",
    "#         r1_kernel_raw = self.conv_r1(z_f)\n",
    "#         cls1_kernel_raw = self.conv_cls1(z_f)\n",
    "#         kernel_size = r1_kernel_raw.data.size()[-1]\n",
    "#         self.r1_kernel = r1_kernel_raw.view(self.anchor*4, self.feature_out, kernel_size, kernel_size)\n",
    "#         self.cls1_kernel = cls1_kernel_raw.view(self.anchor*2, self.feature_out, kernel_size, kernel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc80bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiamRPN()\n",
    "model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "# x = torch.randn(1,3,271,271).cuda()\n",
    "x = torch.randint(255,(1,3,271,271)).float().cuda()\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "ONNX_FILE_PATH = \"SiamRPNOTB.onnx\"\n",
    "\n",
    "# torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"zf\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model,x, ONNX_FILE_PATH, input_names=[\"x\"], \n",
    "                  output_names=[\"delta\", \"score\" ], export_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b75bf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "temple = Temple()\n",
    "temple.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "temple.eval()\n",
    "temple.cuda()\n",
    "r1_kernel,cls1_kernel = temple(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b4127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "siam = SiamRPN()\n",
    "siam.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "siam.eval()\n",
    "siam.cuda()\n",
    "delta,score = siam(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51be0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_in = F.conv2d(delta, r1_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d313e8fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RegressAdjust' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19207/3671417925.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegressAdjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# model.load_state_dict(torch.load('SiamRPNOTB.model'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# delta_ = torch.randn(1,20,19,19).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RegressAdjust' is not defined"
     ]
    }
   ],
   "source": [
    "model = RegressAdjust()\n",
    "# model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "# delta_ = torch.randn(1,20,19,19).cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "ONNX_FILE_PATH = \"RegressAdjust.onnx\"\n",
    "\n",
    "# torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"zf\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model,delta_in, ONNX_FILE_PATH, input_names=[\"delta_in\"], \n",
    "                  output_names=[\"final_delta\" ], export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d373e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a457d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa8df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         return self.regress_adjust(F.conv2d(self.conv_r2(x_f), self.r1_kernel)), \\\n",
    "#                F.conv2d(self.conv_cls2(x_f), self.cls1_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225a251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c552eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressAdjust()\n",
    "# model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "# delta_ = torch.randn(1,20,19,19).cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "ONNX_FILE_PATH = \"RegressAdjust.onnx\"\n",
    "\n",
    "# torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"zf\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model,x, ONNX_FILE_PATH, input_names=[\"delta2\"], \n",
    "                  output_names=[\"final_delta\" ], export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ee859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700d294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c152eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83294bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# DaSiamRPN\n",
    "# Licensed under The MIT License\n",
    "# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n",
    "# --------------------------------------------------------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "size = 1\n",
    "configs = [3, 96, 256, 384, 384, 256]\n",
    "configs = list(map(lambda x: 3 if x==3 else x*size, configs))\n",
    "feat_in = configs[-1]\n",
    "feature_out = 256\n",
    "anchor=5\n",
    "class SiamRPN(nn.Module):\n",
    "    def __init__(self):\n",
    "#     def __init__(self, size=2, feature_out=512, anchor=5):\n",
    "        \n",
    "        super(SiamRPN, self).__init__()\n",
    "        self.featureExtract = nn.Sequential(\n",
    "            nn.Conv2d(configs[0], configs[1] , kernel_size=11, stride=2),\n",
    "            nn.BatchNorm2d(configs[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[1], configs[2], kernel_size=5),\n",
    "            nn.BatchNorm2d(configs[2]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[2], configs[3], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[3]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[3], configs[4], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[4]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[4], configs[5], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[5]),\n",
    "        )\n",
    "\n",
    "        self.anchor = anchor\n",
    "        self.feature_out = feature_out\n",
    "\n",
    "        self.conv_r1 = nn.Conv2d(feat_in, feature_out*4*anchor, 3)\n",
    "        self.conv_r2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.conv_cls1 = nn.Conv2d(feat_in, feature_out*2*anchor, 3)\n",
    "        self.conv_cls2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.regress_adjust = nn.Conv2d(4*anchor, 4*anchor, 1)\n",
    "\n",
    "#         self.r1_kernel = []\n",
    "#         self.cls1_kernel = []\n",
    "\n",
    "        self.cfg = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_f = self.featureExtract(x)\n",
    "        return self.regress_adjust(F.conv2d(self.conv_r2(x_f), self.r1_kernel)), \\\n",
    "               F.conv2d(self.conv_cls2(x_f), self.cls1_kernel)\n",
    "\n",
    "    def temple(self, z):\n",
    "        z_f = self.featureExtract(z)\n",
    "        r1_kernel_raw = self.conv_r1(z_f)\n",
    "        cls1_kernel_raw = self.conv_cls1(z_f)\n",
    "        kernel_size = r1_kernel_raw.data.size()[-1]\n",
    "        self.r1_kernel = r1_kernel_raw.view(self.anchor*4, self.feature_out, kernel_size, kernel_size)\n",
    "        self.cls1_kernel = cls1_kernel_raw.view(self.anchor*2, self.feature_out, kernel_size, kernel_size)\n",
    "\n",
    "\n",
    "class SiamRPNBIG(SiamRPN):\n",
    "    def __init__(self):\n",
    "        super(SiamRPNBIG, self).__init__(size=2) \n",
    "        # self.cfg = {'lr':0.295, 'window_influence': 0.42, 'penalty_k': 0.055, 'instance_size': 271, 'adaptive': True} # 0.383\n",
    "        self.cfg = {'lr':0.295, 'window_influence': 0.42, 'penalty_k': 0.055, 'instance_size': 271, 'adaptive': True} # 0.383\n",
    "\n",
    "\n",
    "class SiamRPNvot(SiamRPN):\n",
    "    def __init__(self):\n",
    "        super(SiamRPNvot, self).__init__(size=1, feature_out=256)\n",
    "        self.cfg = {'lr':0.45, 'window_influence': 0.44, 'penalty_k': 0.04, 'instance_size': 271, 'adaptive': True} # 0.355\n",
    "\n",
    "\n",
    "class SiamRPNotb(SiamRPN):\n",
    "    def __init__(self):\n",
    "        super(SiamRPNotb, self).__init__()#size=1, feature_out=256)\n",
    "        self.cfg = {'lr': 0.30, 'window_influence': 0.40, 'penalty_k': 0.22, 'instance_size': 271, 'adaptive': True} # 0.655\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37faa7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SiamRPNotb()\n",
    "net.eval()\n",
    "net.cuda()\n",
    "z = 255*torch.randn(1,3,127,127).cuda()\n",
    "net.temple(z)\n",
    "x = 255*torch.randn(1,3,271,271).cuda()\n",
    "delta, score = net(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2c4fab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = delta.permute(1, 2, 3, 0).contiguous().view(4, -1).data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ca2e40c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 19, 19])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43865a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3e691239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1805])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta[0, :] = delta[0, :] * p_anchor[:, 2] + p_anchor[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0cab1eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1805)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e231b41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1805,)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_anchor[:,2].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b88c9879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.36012095, -0.36366123, -0.3641756 , ...,  0.13530128,\n",
       "        0.14852202,  0.15459633], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816465b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735c955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3107afd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 19, 19])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "80268878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1805])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score = F.softmax(score.permute(1, 2, 3, 0).contiguous().view(2, -1), dim=0).data[1, :].cpu().numpy()\n",
    "score = F.softmax(score.permute(1, 2, 3, 0).contiguous().view(2, -1), dim=0)[1, :]\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "50e24757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 19, 19])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score1= score.permute(1, 2, 3, 0).contiguous().view(2, -1)\n",
    "score2= score.view(2, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "74dcaf35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0119,  0.0039,  0.0053,  ..., -0.1937, -0.1969, -0.1956],\n",
       "        [-0.3431, -0.3546, -0.3593,  ..., -0.0187, -0.0306, -0.0422]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a8f31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6e493644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2749,  0.2762,  0.2757,  ..., -0.1389, -0.1329, -0.1407],\n",
       "        [-0.1370, -0.1304, -0.1500,  ...,  0.3213,  0.3137,  0.3050]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bf080f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6015, 0.6003, 0.6049,  ..., 0.3869, 0.3902, 0.3904],\n",
       "        [0.3985, 0.3997, 0.3951,  ..., 0.6131, 0.6098, 0.6096]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(score,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e487cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array([0.2749,-0.1370])\n",
    "temp = torch.from_numpy(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5efa22c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6015, 0.3985], dtype=torch.float64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(temp,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc972d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c30b7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236e606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c14f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435de875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# DaSiamRPN\n",
    "# Licensed under The MIT License\n",
    "# Written by Qiang Wang (wangqiang2015 at ia.ac.cn)\n",
    "# --------------------------------------------------------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "size = 1\n",
    "configs = [3, 96, 256, 384, 384, 256]\n",
    "configs = list(map(lambda x: 3 if x==3 else x*size, configs))\n",
    "feat_in = configs[-1]\n",
    "feature_out = 256\n",
    "anchor=5\n",
    "class Temple(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(Temple, self).__init__()\n",
    "        self.cfg = {'lr': 0.30, 'window_influence': 0.40, 'penalty_k': 0.22, 'instance_size': 271, 'adaptive': True} # 0.655\n",
    "\n",
    "        self.featureExtract = nn.Sequential(\n",
    "            nn.Conv2d(configs[0], configs[1] , kernel_size=11, stride=2),\n",
    "            nn.BatchNorm2d(configs[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[1], configs[2], kernel_size=5),\n",
    "            nn.BatchNorm2d(configs[2]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[2], configs[3], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[3]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[3], configs[4], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[4]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[4], configs[5], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[5]),\n",
    "        )\n",
    "\n",
    "        self.anchor = anchor\n",
    "        self.feature_out = feature_out\n",
    "\n",
    "        self.conv_r1 = nn.Conv2d(feat_in, feature_out*4*anchor, 3)\n",
    "        self.conv_r2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.conv_cls1 = nn.Conv2d(feat_in, feature_out*2*anchor, 3)\n",
    "        self.conv_cls2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.regress_adjust = nn.Conv2d(4*anchor, 4*anchor, 1)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, z):\n",
    "        with torch.no_grad():\n",
    "            z_f = self.featureExtract(z)\n",
    "            r1_kernel_raw = self.conv_r1(z_f)\n",
    "            cls1_kernel_raw = self.conv_cls1(z_f)\n",
    "            kernel_size = r1_kernel_raw.data.size()[-1]\n",
    "            r1_kernel = r1_kernel_raw.view(self.anchor*4, self.feature_out, kernel_size, kernel_size)\n",
    "            cls1_kernel = cls1_kernel_raw.view(self.anchor*2, self.feature_out, kernel_size, kernel_size)\n",
    "        return r1_kernel,cls1_kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f749b8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Temple()\n",
    "model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "z = torch.randn(1,3,127,127).cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "ONNX_FILE_PATH = \"temple.onnx\"\n",
    "\n",
    "# torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"zf\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"r1_kernel\",\"cls1_kernel\"], export_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9386e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_kernel,cls1_kernel = model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = 1\n",
    "configs = [3, 96, 256, 384, 384, 256]\n",
    "configs = list(map(lambda x: 3 if x==3 else x*size, configs))\n",
    "feat_in = configs[-1]\n",
    "feature_out = 256\n",
    "anchor=5\n",
    "class SiamRPN(nn.Module):\n",
    "    def __init__(self):\n",
    "#     def __init__(self, size=2, feature_out=512, anchor=5):\n",
    "        \n",
    "        super(SiamRPN, self).__init__()\n",
    "        self.featureExtract = nn.Sequential(\n",
    "            nn.Conv2d(configs[0], configs[1] , kernel_size=11, stride=2),\n",
    "            nn.BatchNorm2d(configs[1]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[1], configs[2], kernel_size=5),\n",
    "            nn.BatchNorm2d(configs[2]),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[2], configs[3], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[3]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[3], configs[4], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[4]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(configs[4], configs[5], kernel_size=3),\n",
    "            nn.BatchNorm2d(configs[5]),\n",
    "        )\n",
    "\n",
    "        self.anchor = anchor\n",
    "        self.feature_out = feature_out\n",
    "\n",
    "        self.conv_r1 = nn.Conv2d(feat_in, feature_out*4*anchor, 3)\n",
    "        self.conv_r2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.conv_cls1 = nn.Conv2d(feat_in, feature_out*2*anchor, 3)\n",
    "        self.conv_cls2 = nn.Conv2d(feat_in, feature_out, 3)\n",
    "        self.regress_adjust = nn.Conv2d(4*anchor, 4*anchor, 1)\n",
    "\n",
    "#         self.r1_kernel = []\n",
    "#         self.cls1_kernel = []\n",
    "\n",
    "        self.cfg = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_f = self.featureExtract(x)\n",
    "        return self.conv_r2(x_f),self.conv_cls2(x_f)\n",
    "#         return self.regress_adjust(F.conv2d(self.conv_r2(x_f), self.r1_kernel)), \\\n",
    "#                F.conv2d(self.conv_cls2(x_f), self.cls1_kernel)\n",
    "    \n",
    "        \n",
    "#     def temple(self, z):\n",
    "#         z_f = self.featureExtract(z)\n",
    "#         r1_kernel_raw = self.conv_r1(z_f)\n",
    "#         cls1_kernel_raw = self.conv_cls1(z_f)\n",
    "#         kernel_size = r1_kernel_raw.data.size()[-1]\n",
    "#         self.r1_kernel = r1_kernel_raw.view(self.anchor*4, self.feature_out, kernel_size, kernel_size)\n",
    "#         self.cls1_kernel = cls1_kernel_raw.view(self.anchor*2, self.feature_out, kernel_size, kernel_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4487df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiamRPN()\n",
    "model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "x = torch.randn(1,3,271,271).cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "ONNX_FILE_PATH = \"SiamRPNOTB.onnx\"\n",
    "\n",
    "# torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"zf\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model,x, ONNX_FILE_PATH, input_names=[\"x\"], \n",
    "                  output_names=[\"delta\", \"score\" ], export_params=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e735c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe8b05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.ones(1,256,22,22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44367eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernle = torch.ones(10,256,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6737b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = F.conv2d(inp,kernle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731b04fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 19, 19])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80d66dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4096.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb7e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "684c53a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temple = Temple()\n",
    "temple = Temple()\n",
    "temple.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "z = torch.randn(1,3,127,127).cuda()\n",
    "temple.eval()\n",
    "temple.cuda()\n",
    "r1_kernel,cls1_kernel = temple(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ccef7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "siam = SiamRPN()\n",
    "siam.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "x = torch.randn(1,3,271,271).cuda()\n",
    "siam.eval()\n",
    "siam.cuda()\n",
    "delta,score = siam(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a04e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 22, 22])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "015fc36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 22, 22])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ed01a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 256, 4, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b7bcdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 4, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls1_kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta2 = F.conv2d(delta, r1_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9ad55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3d1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7081eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta2 = F.conv2d(delta, r1_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5b2cc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 19, 19])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2 = F.conv2d(score, cls1_kernel)\n",
    "score2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5cf5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 19, 19])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a049ed27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 256, 4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91ed734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 4, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls1_kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e858301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressAdjust(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressAdjust, self).__init__()\n",
    "        self.conv = siam.regress_adjust\n",
    "    def forward(self,delta2):\n",
    "        delta = self.conv(delta2)\n",
    "        delta = delta.permute(1, 2, 3, 0).contiguous().view(4, -1)\n",
    "        return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "04c0ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressAdjust()\n",
    "# model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "x = torch.randn(1,20,19,19).cuda()\n",
    "model.eval()\n",
    "model.cuda()\n",
    "ONNX_FILE_PATH = \"RegressAdjust.onnx\"\n",
    "\n",
    "# torch.onnx.export(model, z, ONNX_FILE_PATH, input_names=[\"z\"], output_names=[\"zf\"], export_params=True)\n",
    "\n",
    "torch.onnx.export(model,x, ONNX_FILE_PATH, input_names=[\"delta2\"], \n",
    "                  output_names=[\"final_delta\" ], export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5b25d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressAdjust(\n",
       "  (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RegressAdjust()\n",
    "# model.load_state_dict(torch.load('SiamRPNOTB.model'))\n",
    "x = torch.ones(1,20,19,19).cuda()\n",
    "model.eval()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a51429dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc25124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0188,  0.0188,  0.0188,  ..., -0.0174, -0.0174, -0.0174],\n",
       "        [-0.0543, -0.0543, -0.0543,  ..., -0.1321, -0.1321, -0.1321],\n",
       "        [-0.3140, -0.3140, -0.3140,  ...,  0.3735,  0.3735,  0.3735],\n",
       "        [ 0.3745,  0.3745,  0.3745,  ..., -0.0428, -0.0428, -0.0428]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6e6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9e85e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1305769241.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_31539/1305769241.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -0.0615267 -0.099464 -0.196093 0.289468\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-0.0615267 -0.099464 -0.196093 0.289468 \n",
    "\n",
    "------------------------------------------------------\n",
    "-2.39877 0.817151 85.4813 42.7429 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b2f7406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1805/19/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b51b894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor(total_stride, scales, ratios, score_size):\n",
    "    anchor_num = len(ratios) * len(scales)\n",
    "    anchor = np.zeros((anchor_num, 4),  dtype=np.float32)\n",
    "    size = total_stride * total_stride\n",
    "    count = 0\n",
    "    for ratio in ratios:\n",
    "        # ws = int(np.sqrt(size * 1.0 / ratio))\n",
    "        ws = int(np.sqrt(size / ratio))\n",
    "        hs = int(ws * ratio)\n",
    "        for scale in scales:\n",
    "            wws = ws * scale\n",
    "            hhs = hs * scale\n",
    "            anchor[count, 0] = 0\n",
    "            anchor[count, 1] = 0\n",
    "            anchor[count, 2] = wws\n",
    "            anchor[count, 3] = hhs\n",
    "            count += 1\n",
    "\n",
    "    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n",
    "    ori = - (score_size / 2) * total_stride\n",
    "    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n",
    "                         [ori + total_stride * dy for dy in range(score_size)])\n",
    "    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n",
    "             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n",
    "    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n",
    "    return anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd950884",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_size = (271 - 127) / 8 + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e153d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = generate_anchor(8,[8, ], [0.33, 0.5, 1, 2, 3], int(score_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ec0bdd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-76. -76. 104.  32.]\n",
      "[-68. -76. 104.  32.]\n",
      "[-60. -76. 104.  32.]\n",
      "[-52. -76. 104.  32.]\n",
      "[-44. -76. 104.  32.]\n",
      "[-36. -76. 104.  32.]\n",
      "[-28. -76. 104.  32.]\n",
      "[-20. -76. 104.  32.]\n",
      "[-12. -76. 104.  32.]\n",
      "[ -4. -76. 104.  32.]\n",
      "[  4. -76. 104.  32.]\n",
      "[ 12. -76. 104.  32.]\n",
      "[ 20. -76. 104.  32.]\n",
      "[ 28. -76. 104.  32.]\n",
      "[ 36. -76. 104.  32.]\n",
      "[ 44. -76. 104.  32.]\n",
      "[ 52. -76. 104.  32.]\n",
      "[ 60. -76. 104.  32.]\n",
      "[ 68. -76. 104.  32.]\n",
      "[-76. -68. 104.  32.]\n",
      "[-68. -68. 104.  32.]\n",
      "[-60. -68. 104.  32.]\n",
      "[-52. -68. 104.  32.]\n",
      "[-44. -68. 104.  32.]\n",
      "[-36. -68. 104.  32.]\n",
      "[-28. -68. 104.  32.]\n",
      "[-20. -68. 104.  32.]\n",
      "[-12. -68. 104.  32.]\n",
      "[ -4. -68. 104.  32.]\n",
      "[  4. -68. 104.  32.]\n",
      "[ 12. -68. 104.  32.]\n",
      "[ 20. -68. 104.  32.]\n",
      "[ 28. -68. 104.  32.]\n",
      "[ 36. -68. 104.  32.]\n",
      "[ 44. -68. 104.  32.]\n",
      "[ 52. -68. 104.  32.]\n",
      "[ 60. -68. 104.  32.]\n",
      "[ 68. -68. 104.  32.]\n",
      "[-76. -60. 104.  32.]\n",
      "[-68. -60. 104.  32.]\n",
      "[-60. -60. 104.  32.]\n",
      "[-52. -60. 104.  32.]\n",
      "[-44. -60. 104.  32.]\n",
      "[-36. -60. 104.  32.]\n",
      "[-28. -60. 104.  32.]\n",
      "[-20. -60. 104.  32.]\n",
      "[-12. -60. 104.  32.]\n",
      "[ -4. -60. 104.  32.]\n",
      "[  4. -60. 104.  32.]\n",
      "[ 12. -60. 104.  32.]\n",
      "[ 20. -60. 104.  32.]\n",
      "[ 28. -60. 104.  32.]\n",
      "[ 36. -60. 104.  32.]\n",
      "[ 44. -60. 104.  32.]\n",
      "[ 52. -60. 104.  32.]\n",
      "[ 60. -60. 104.  32.]\n",
      "[ 68. -60. 104.  32.]\n",
      "[-76. -52. 104.  32.]\n",
      "[-68. -52. 104.  32.]\n",
      "[-60. -52. 104.  32.]\n",
      "[-52. -52. 104.  32.]\n",
      "[-44. -52. 104.  32.]\n",
      "[-36. -52. 104.  32.]\n",
      "[-28. -52. 104.  32.]\n",
      "[-20. -52. 104.  32.]\n",
      "[-12. -52. 104.  32.]\n",
      "[ -4. -52. 104.  32.]\n",
      "[  4. -52. 104.  32.]\n",
      "[ 12. -52. 104.  32.]\n",
      "[ 20. -52. 104.  32.]\n",
      "[ 28. -52. 104.  32.]\n",
      "[ 36. -52. 104.  32.]\n",
      "[ 44. -52. 104.  32.]\n",
      "[ 52. -52. 104.  32.]\n",
      "[ 60. -52. 104.  32.]\n",
      "[ 68. -52. 104.  32.]\n",
      "[-76. -44. 104.  32.]\n",
      "[-68. -44. 104.  32.]\n",
      "[-60. -44. 104.  32.]\n",
      "[-52. -44. 104.  32.]\n",
      "[-44. -44. 104.  32.]\n",
      "[-36. -44. 104.  32.]\n",
      "[-28. -44. 104.  32.]\n",
      "[-20. -44. 104.  32.]\n",
      "[-12. -44. 104.  32.]\n",
      "[ -4. -44. 104.  32.]\n",
      "[  4. -44. 104.  32.]\n",
      "[ 12. -44. 104.  32.]\n",
      "[ 20. -44. 104.  32.]\n",
      "[ 28. -44. 104.  32.]\n",
      "[ 36. -44. 104.  32.]\n",
      "[ 44. -44. 104.  32.]\n",
      "[ 52. -44. 104.  32.]\n",
      "[ 60. -44. 104.  32.]\n",
      "[ 68. -44. 104.  32.]\n",
      "[-76. -36. 104.  32.]\n",
      "[-68. -36. 104.  32.]\n",
      "[-60. -36. 104.  32.]\n",
      "[-52. -36. 104.  32.]\n",
      "[-44. -36. 104.  32.]\n",
      "[-36. -36. 104.  32.]\n",
      "[-28. -36. 104.  32.]\n",
      "[-20. -36. 104.  32.]\n",
      "[-12. -36. 104.  32.]\n",
      "[ -4. -36. 104.  32.]\n",
      "[  4. -36. 104.  32.]\n",
      "[ 12. -36. 104.  32.]\n",
      "[ 20. -36. 104.  32.]\n",
      "[ 28. -36. 104.  32.]\n",
      "[ 36. -36. 104.  32.]\n",
      "[ 44. -36. 104.  32.]\n",
      "[ 52. -36. 104.  32.]\n",
      "[ 60. -36. 104.  32.]\n",
      "[ 68. -36. 104.  32.]\n",
      "[-76. -28. 104.  32.]\n",
      "[-68. -28. 104.  32.]\n",
      "[-60. -28. 104.  32.]\n",
      "[-52. -28. 104.  32.]\n",
      "[-44. -28. 104.  32.]\n",
      "[-36. -28. 104.  32.]\n",
      "[-28. -28. 104.  32.]\n",
      "[-20. -28. 104.  32.]\n",
      "[-12. -28. 104.  32.]\n",
      "[ -4. -28. 104.  32.]\n",
      "[  4. -28. 104.  32.]\n",
      "[ 12. -28. 104.  32.]\n",
      "[ 20. -28. 104.  32.]\n",
      "[ 28. -28. 104.  32.]\n",
      "[ 36. -28. 104.  32.]\n",
      "[ 44. -28. 104.  32.]\n",
      "[ 52. -28. 104.  32.]\n",
      "[ 60. -28. 104.  32.]\n",
      "[ 68. -28. 104.  32.]\n",
      "[-76. -20. 104.  32.]\n",
      "[-68. -20. 104.  32.]\n",
      "[-60. -20. 104.  32.]\n",
      "[-52. -20. 104.  32.]\n",
      "[-44. -20. 104.  32.]\n",
      "[-36. -20. 104.  32.]\n",
      "[-28. -20. 104.  32.]\n",
      "[-20. -20. 104.  32.]\n",
      "[-12. -20. 104.  32.]\n",
      "[ -4. -20. 104.  32.]\n",
      "[  4. -20. 104.  32.]\n",
      "[ 12. -20. 104.  32.]\n",
      "[ 20. -20. 104.  32.]\n",
      "[ 28. -20. 104.  32.]\n",
      "[ 36. -20. 104.  32.]\n",
      "[ 44. -20. 104.  32.]\n",
      "[ 52. -20. 104.  32.]\n",
      "[ 60. -20. 104.  32.]\n",
      "[ 68. -20. 104.  32.]\n",
      "[-76. -12. 104.  32.]\n",
      "[-68. -12. 104.  32.]\n",
      "[-60. -12. 104.  32.]\n",
      "[-52. -12. 104.  32.]\n",
      "[-44. -12. 104.  32.]\n",
      "[-36. -12. 104.  32.]\n",
      "[-28. -12. 104.  32.]\n",
      "[-20. -12. 104.  32.]\n",
      "[-12. -12. 104.  32.]\n",
      "[ -4. -12. 104.  32.]\n",
      "[  4. -12. 104.  32.]\n",
      "[ 12. -12. 104.  32.]\n",
      "[ 20. -12. 104.  32.]\n",
      "[ 28. -12. 104.  32.]\n",
      "[ 36. -12. 104.  32.]\n",
      "[ 44. -12. 104.  32.]\n",
      "[ 52. -12. 104.  32.]\n",
      "[ 60. -12. 104.  32.]\n",
      "[ 68. -12. 104.  32.]\n",
      "[-76.  -4. 104.  32.]\n",
      "[-68.  -4. 104.  32.]\n",
      "[-60.  -4. 104.  32.]\n",
      "[-52.  -4. 104.  32.]\n",
      "[-44.  -4. 104.  32.]\n",
      "[-36.  -4. 104.  32.]\n",
      "[-28.  -4. 104.  32.]\n",
      "[-20.  -4. 104.  32.]\n",
      "[-12.  -4. 104.  32.]\n",
      "[ -4.  -4. 104.  32.]\n",
      "[  4.  -4. 104.  32.]\n",
      "[ 12.  -4. 104.  32.]\n",
      "[ 20.  -4. 104.  32.]\n",
      "[ 28.  -4. 104.  32.]\n",
      "[ 36.  -4. 104.  32.]\n",
      "[ 44.  -4. 104.  32.]\n",
      "[ 52.  -4. 104.  32.]\n",
      "[ 60.  -4. 104.  32.]\n",
      "[ 68.  -4. 104.  32.]\n",
      "[-76.   4. 104.  32.]\n",
      "[-68.   4. 104.  32.]\n",
      "[-60.   4. 104.  32.]\n",
      "[-52.   4. 104.  32.]\n",
      "[-44.   4. 104.  32.]\n",
      "[-36.   4. 104.  32.]\n",
      "[-28.   4. 104.  32.]\n",
      "[-20.   4. 104.  32.]\n",
      "[-12.   4. 104.  32.]\n",
      "[ -4.   4. 104.  32.]\n",
      "[  4.   4. 104.  32.]\n",
      "[ 12.   4. 104.  32.]\n",
      "[ 20.   4. 104.  32.]\n",
      "[ 28.   4. 104.  32.]\n",
      "[ 36.   4. 104.  32.]\n",
      "[ 44.   4. 104.  32.]\n",
      "[ 52.   4. 104.  32.]\n",
      "[ 60.   4. 104.  32.]\n",
      "[ 68.   4. 104.  32.]\n",
      "[-76.  12. 104.  32.]\n",
      "[-68.  12. 104.  32.]\n",
      "[-60.  12. 104.  32.]\n",
      "[-52.  12. 104.  32.]\n",
      "[-44.  12. 104.  32.]\n",
      "[-36.  12. 104.  32.]\n",
      "[-28.  12. 104.  32.]\n",
      "[-20.  12. 104.  32.]\n",
      "[-12.  12. 104.  32.]\n",
      "[ -4.  12. 104.  32.]\n",
      "[  4.  12. 104.  32.]\n",
      "[ 12.  12. 104.  32.]\n",
      "[ 20.  12. 104.  32.]\n",
      "[ 28.  12. 104.  32.]\n",
      "[ 36.  12. 104.  32.]\n",
      "[ 44.  12. 104.  32.]\n",
      "[ 52.  12. 104.  32.]\n",
      "[ 60.  12. 104.  32.]\n",
      "[ 68.  12. 104.  32.]\n",
      "[-76.  20. 104.  32.]\n",
      "[-68.  20. 104.  32.]\n",
      "[-60.  20. 104.  32.]\n",
      "[-52.  20. 104.  32.]\n",
      "[-44.  20. 104.  32.]\n",
      "[-36.  20. 104.  32.]\n",
      "[-28.  20. 104.  32.]\n",
      "[-20.  20. 104.  32.]\n",
      "[-12.  20. 104.  32.]\n",
      "[ -4.  20. 104.  32.]\n",
      "[  4.  20. 104.  32.]\n",
      "[ 12.  20. 104.  32.]\n",
      "[ 20.  20. 104.  32.]\n",
      "[ 28.  20. 104.  32.]\n",
      "[ 36.  20. 104.  32.]\n",
      "[ 44.  20. 104.  32.]\n",
      "[ 52.  20. 104.  32.]\n",
      "[ 60.  20. 104.  32.]\n",
      "[ 68.  20. 104.  32.]\n",
      "[-76.  28. 104.  32.]\n",
      "[-68.  28. 104.  32.]\n",
      "[-60.  28. 104.  32.]\n",
      "[-52.  28. 104.  32.]\n",
      "[-44.  28. 104.  32.]\n",
      "[-36.  28. 104.  32.]\n",
      "[-28.  28. 104.  32.]\n",
      "[-20.  28. 104.  32.]\n",
      "[-12.  28. 104.  32.]\n",
      "[ -4.  28. 104.  32.]\n",
      "[  4.  28. 104.  32.]\n",
      "[ 12.  28. 104.  32.]\n",
      "[ 20.  28. 104.  32.]\n",
      "[ 28.  28. 104.  32.]\n",
      "[ 36.  28. 104.  32.]\n",
      "[ 44.  28. 104.  32.]\n",
      "[ 52.  28. 104.  32.]\n",
      "[ 60.  28. 104.  32.]\n",
      "[ 68.  28. 104.  32.]\n",
      "[-76.  36. 104.  32.]\n",
      "[-68.  36. 104.  32.]\n",
      "[-60.  36. 104.  32.]\n",
      "[-52.  36. 104.  32.]\n",
      "[-44.  36. 104.  32.]\n",
      "[-36.  36. 104.  32.]\n",
      "[-28.  36. 104.  32.]\n",
      "[-20.  36. 104.  32.]\n",
      "[-12.  36. 104.  32.]\n",
      "[ -4.  36. 104.  32.]\n",
      "[  4.  36. 104.  32.]\n",
      "[ 12.  36. 104.  32.]\n",
      "[ 20.  36. 104.  32.]\n",
      "[ 28.  36. 104.  32.]\n",
      "[ 36.  36. 104.  32.]\n",
      "[ 44.  36. 104.  32.]\n",
      "[ 52.  36. 104.  32.]\n",
      "[ 60.  36. 104.  32.]\n",
      "[ 68.  36. 104.  32.]\n",
      "[-76.  44. 104.  32.]\n",
      "[-68.  44. 104.  32.]\n",
      "[-60.  44. 104.  32.]\n",
      "[-52.  44. 104.  32.]\n",
      "[-44.  44. 104.  32.]\n",
      "[-36.  44. 104.  32.]\n",
      "[-28.  44. 104.  32.]\n",
      "[-20.  44. 104.  32.]\n",
      "[-12.  44. 104.  32.]\n",
      "[ -4.  44. 104.  32.]\n",
      "[  4.  44. 104.  32.]\n",
      "[ 12.  44. 104.  32.]\n",
      "[ 20.  44. 104.  32.]\n",
      "[ 28.  44. 104.  32.]\n",
      "[ 36.  44. 104.  32.]\n",
      "[ 44.  44. 104.  32.]\n",
      "[ 52.  44. 104.  32.]\n",
      "[ 60.  44. 104.  32.]\n",
      "[ 68.  44. 104.  32.]\n",
      "[-76.  52. 104.  32.]\n",
      "[-68.  52. 104.  32.]\n",
      "[-60.  52. 104.  32.]\n",
      "[-52.  52. 104.  32.]\n",
      "[-44.  52. 104.  32.]\n",
      "[-36.  52. 104.  32.]\n",
      "[-28.  52. 104.  32.]\n",
      "[-20.  52. 104.  32.]\n",
      "[-12.  52. 104.  32.]\n",
      "[ -4.  52. 104.  32.]\n",
      "[  4.  52. 104.  32.]\n",
      "[ 12.  52. 104.  32.]\n",
      "[ 20.  52. 104.  32.]\n",
      "[ 28.  52. 104.  32.]\n",
      "[ 36.  52. 104.  32.]\n",
      "[ 44.  52. 104.  32.]\n",
      "[ 52.  52. 104.  32.]\n",
      "[ 60.  52. 104.  32.]\n",
      "[ 68.  52. 104.  32.]\n",
      "[-76.  60. 104.  32.]\n",
      "[-68.  60. 104.  32.]\n",
      "[-60.  60. 104.  32.]\n",
      "[-52.  60. 104.  32.]\n",
      "[-44.  60. 104.  32.]\n",
      "[-36.  60. 104.  32.]\n",
      "[-28.  60. 104.  32.]\n",
      "[-20.  60. 104.  32.]\n",
      "[-12.  60. 104.  32.]\n",
      "[ -4.  60. 104.  32.]\n",
      "[  4.  60. 104.  32.]\n",
      "[ 12.  60. 104.  32.]\n",
      "[ 20.  60. 104.  32.]\n",
      "[ 28.  60. 104.  32.]\n",
      "[ 36.  60. 104.  32.]\n",
      "[ 44.  60. 104.  32.]\n",
      "[ 52.  60. 104.  32.]\n",
      "[ 60.  60. 104.  32.]\n",
      "[ 68.  60. 104.  32.]\n",
      "[-76.  68. 104.  32.]\n",
      "[-68.  68. 104.  32.]\n",
      "[-60.  68. 104.  32.]\n",
      "[-52.  68. 104.  32.]\n",
      "[-44.  68. 104.  32.]\n",
      "[-36.  68. 104.  32.]\n",
      "[-28.  68. 104.  32.]\n",
      "[-20.  68. 104.  32.]\n",
      "[-12.  68. 104.  32.]\n",
      "[ -4.  68. 104.  32.]\n",
      "[  4.  68. 104.  32.]\n",
      "[ 12.  68. 104.  32.]\n",
      "[ 20.  68. 104.  32.]\n",
      "[ 28.  68. 104.  32.]\n",
      "[ 36.  68. 104.  32.]\n",
      "[ 44.  68. 104.  32.]\n",
      "[ 52.  68. 104.  32.]\n",
      "[ 60.  68. 104.  32.]\n",
      "[ 68.  68. 104.  32.]\n",
      "[-76. -76.  88.  40.]\n",
      "[-68. -76.  88.  40.]\n",
      "[-60. -76.  88.  40.]\n",
      "[-52. -76.  88.  40.]\n",
      "[-44. -76.  88.  40.]\n",
      "[-36. -76.  88.  40.]\n",
      "[-28. -76.  88.  40.]\n",
      "[-20. -76.  88.  40.]\n",
      "[-12. -76.  88.  40.]\n",
      "[ -4. -76.  88.  40.]\n",
      "[  4. -76.  88.  40.]\n",
      "[ 12. -76.  88.  40.]\n",
      "[ 20. -76.  88.  40.]\n",
      "[ 28. -76.  88.  40.]\n",
      "[ 36. -76.  88.  40.]\n",
      "[ 44. -76.  88.  40.]\n",
      "[ 52. -76.  88.  40.]\n",
      "[ 60. -76.  88.  40.]\n",
      "[ 68. -76.  88.  40.]\n",
      "[-76. -68.  88.  40.]\n",
      "[-68. -68.  88.  40.]\n",
      "[-60. -68.  88.  40.]\n",
      "[-52. -68.  88.  40.]\n",
      "[-44. -68.  88.  40.]\n",
      "[-36. -68.  88.  40.]\n",
      "[-28. -68.  88.  40.]\n",
      "[-20. -68.  88.  40.]\n",
      "[-12. -68.  88.  40.]\n",
      "[ -4. -68.  88.  40.]\n",
      "[  4. -68.  88.  40.]\n",
      "[ 12. -68.  88.  40.]\n",
      "[ 20. -68.  88.  40.]\n",
      "[ 28. -68.  88.  40.]\n",
      "[ 36. -68.  88.  40.]\n",
      "[ 44. -68.  88.  40.]\n",
      "[ 52. -68.  88.  40.]\n",
      "[ 60. -68.  88.  40.]\n",
      "[ 68. -68.  88.  40.]\n",
      "[-76. -60.  88.  40.]\n",
      "[-68. -60.  88.  40.]\n",
      "[-60. -60.  88.  40.]\n",
      "[-52. -60.  88.  40.]\n",
      "[-44. -60.  88.  40.]\n",
      "[-36. -60.  88.  40.]\n",
      "[-28. -60.  88.  40.]\n",
      "[-20. -60.  88.  40.]\n",
      "[-12. -60.  88.  40.]\n",
      "[ -4. -60.  88.  40.]\n",
      "[  4. -60.  88.  40.]\n",
      "[ 12. -60.  88.  40.]\n",
      "[ 20. -60.  88.  40.]\n",
      "[ 28. -60.  88.  40.]\n",
      "[ 36. -60.  88.  40.]\n",
      "[ 44. -60.  88.  40.]\n",
      "[ 52. -60.  88.  40.]\n",
      "[ 60. -60.  88.  40.]\n",
      "[ 68. -60.  88.  40.]\n",
      "[-76. -52.  88.  40.]\n",
      "[-68. -52.  88.  40.]\n",
      "[-60. -52.  88.  40.]\n",
      "[-52. -52.  88.  40.]\n",
      "[-44. -52.  88.  40.]\n",
      "[-36. -52.  88.  40.]\n",
      "[-28. -52.  88.  40.]\n",
      "[-20. -52.  88.  40.]\n",
      "[-12. -52.  88.  40.]\n",
      "[ -4. -52.  88.  40.]\n",
      "[  4. -52.  88.  40.]\n",
      "[ 12. -52.  88.  40.]\n",
      "[ 20. -52.  88.  40.]\n",
      "[ 28. -52.  88.  40.]\n",
      "[ 36. -52.  88.  40.]\n",
      "[ 44. -52.  88.  40.]\n",
      "[ 52. -52.  88.  40.]\n",
      "[ 60. -52.  88.  40.]\n",
      "[ 68. -52.  88.  40.]\n",
      "[-76. -44.  88.  40.]\n",
      "[-68. -44.  88.  40.]\n",
      "[-60. -44.  88.  40.]\n",
      "[-52. -44.  88.  40.]\n",
      "[-44. -44.  88.  40.]\n",
      "[-36. -44.  88.  40.]\n",
      "[-28. -44.  88.  40.]\n",
      "[-20. -44.  88.  40.]\n",
      "[-12. -44.  88.  40.]\n",
      "[ -4. -44.  88.  40.]\n",
      "[  4. -44.  88.  40.]\n",
      "[ 12. -44.  88.  40.]\n",
      "[ 20. -44.  88.  40.]\n",
      "[ 28. -44.  88.  40.]\n",
      "[ 36. -44.  88.  40.]\n",
      "[ 44. -44.  88.  40.]\n",
      "[ 52. -44.  88.  40.]\n",
      "[ 60. -44.  88.  40.]\n",
      "[ 68. -44.  88.  40.]\n",
      "[-76. -36.  88.  40.]\n",
      "[-68. -36.  88.  40.]\n",
      "[-60. -36.  88.  40.]\n",
      "[-52. -36.  88.  40.]\n",
      "[-44. -36.  88.  40.]\n",
      "[-36. -36.  88.  40.]\n",
      "[-28. -36.  88.  40.]\n",
      "[-20. -36.  88.  40.]\n",
      "[-12. -36.  88.  40.]\n",
      "[ -4. -36.  88.  40.]\n",
      "[  4. -36.  88.  40.]\n",
      "[ 12. -36.  88.  40.]\n",
      "[ 20. -36.  88.  40.]\n",
      "[ 28. -36.  88.  40.]\n",
      "[ 36. -36.  88.  40.]\n",
      "[ 44. -36.  88.  40.]\n",
      "[ 52. -36.  88.  40.]\n",
      "[ 60. -36.  88.  40.]\n",
      "[ 68. -36.  88.  40.]\n",
      "[-76. -28.  88.  40.]\n",
      "[-68. -28.  88.  40.]\n",
      "[-60. -28.  88.  40.]\n",
      "[-52. -28.  88.  40.]\n",
      "[-44. -28.  88.  40.]\n",
      "[-36. -28.  88.  40.]\n",
      "[-28. -28.  88.  40.]\n",
      "[-20. -28.  88.  40.]\n",
      "[-12. -28.  88.  40.]\n",
      "[ -4. -28.  88.  40.]\n",
      "[  4. -28.  88.  40.]\n",
      "[ 12. -28.  88.  40.]\n",
      "[ 20. -28.  88.  40.]\n",
      "[ 28. -28.  88.  40.]\n",
      "[ 36. -28.  88.  40.]\n",
      "[ 44. -28.  88.  40.]\n",
      "[ 52. -28.  88.  40.]\n",
      "[ 60. -28.  88.  40.]\n",
      "[ 68. -28.  88.  40.]\n",
      "[-76. -20.  88.  40.]\n",
      "[-68. -20.  88.  40.]\n",
      "[-60. -20.  88.  40.]\n",
      "[-52. -20.  88.  40.]\n",
      "[-44. -20.  88.  40.]\n",
      "[-36. -20.  88.  40.]\n",
      "[-28. -20.  88.  40.]\n",
      "[-20. -20.  88.  40.]\n",
      "[-12. -20.  88.  40.]\n",
      "[ -4. -20.  88.  40.]\n",
      "[  4. -20.  88.  40.]\n",
      "[ 12. -20.  88.  40.]\n",
      "[ 20. -20.  88.  40.]\n",
      "[ 28. -20.  88.  40.]\n",
      "[ 36. -20.  88.  40.]\n",
      "[ 44. -20.  88.  40.]\n",
      "[ 52. -20.  88.  40.]\n",
      "[ 60. -20.  88.  40.]\n",
      "[ 68. -20.  88.  40.]\n",
      "[-76. -12.  88.  40.]\n",
      "[-68. -12.  88.  40.]\n",
      "[-60. -12.  88.  40.]\n",
      "[-52. -12.  88.  40.]\n",
      "[-44. -12.  88.  40.]\n",
      "[-36. -12.  88.  40.]\n",
      "[-28. -12.  88.  40.]\n",
      "[-20. -12.  88.  40.]\n",
      "[-12. -12.  88.  40.]\n",
      "[ -4. -12.  88.  40.]\n",
      "[  4. -12.  88.  40.]\n",
      "[ 12. -12.  88.  40.]\n",
      "[ 20. -12.  88.  40.]\n",
      "[ 28. -12.  88.  40.]\n",
      "[ 36. -12.  88.  40.]\n",
      "[ 44. -12.  88.  40.]\n",
      "[ 52. -12.  88.  40.]\n",
      "[ 60. -12.  88.  40.]\n",
      "[ 68. -12.  88.  40.]\n",
      "[-76.  -4.  88.  40.]\n",
      "[-68.  -4.  88.  40.]\n",
      "[-60.  -4.  88.  40.]\n",
      "[-52.  -4.  88.  40.]\n",
      "[-44.  -4.  88.  40.]\n",
      "[-36.  -4.  88.  40.]\n",
      "[-28.  -4.  88.  40.]\n",
      "[-20.  -4.  88.  40.]\n",
      "[-12.  -4.  88.  40.]\n",
      "[-4. -4. 88. 40.]\n",
      "[ 4. -4. 88. 40.]\n",
      "[12. -4. 88. 40.]\n",
      "[20. -4. 88. 40.]\n",
      "[28. -4. 88. 40.]\n",
      "[36. -4. 88. 40.]\n",
      "[44. -4. 88. 40.]\n",
      "[52. -4. 88. 40.]\n",
      "[60. -4. 88. 40.]\n",
      "[68. -4. 88. 40.]\n",
      "[-76.   4.  88.  40.]\n",
      "[-68.   4.  88.  40.]\n",
      "[-60.   4.  88.  40.]\n",
      "[-52.   4.  88.  40.]\n",
      "[-44.   4.  88.  40.]\n",
      "[-36.   4.  88.  40.]\n",
      "[-28.   4.  88.  40.]\n",
      "[-20.   4.  88.  40.]\n",
      "[-12.   4.  88.  40.]\n",
      "[-4.  4. 88. 40.]\n",
      "[ 4.  4. 88. 40.]\n",
      "[12.  4. 88. 40.]\n",
      "[20.  4. 88. 40.]\n",
      "[28.  4. 88. 40.]\n",
      "[36.  4. 88. 40.]\n",
      "[44.  4. 88. 40.]\n",
      "[52.  4. 88. 40.]\n",
      "[60.  4. 88. 40.]\n",
      "[68.  4. 88. 40.]\n",
      "[-76.  12.  88.  40.]\n",
      "[-68.  12.  88.  40.]\n",
      "[-60.  12.  88.  40.]\n",
      "[-52.  12.  88.  40.]\n",
      "[-44.  12.  88.  40.]\n",
      "[-36.  12.  88.  40.]\n",
      "[-28.  12.  88.  40.]\n",
      "[-20.  12.  88.  40.]\n",
      "[-12.  12.  88.  40.]\n",
      "[-4. 12. 88. 40.]\n",
      "[ 4. 12. 88. 40.]\n",
      "[12. 12. 88. 40.]\n",
      "[20. 12. 88. 40.]\n",
      "[28. 12. 88. 40.]\n",
      "[36. 12. 88. 40.]\n",
      "[44. 12. 88. 40.]\n",
      "[52. 12. 88. 40.]\n",
      "[60. 12. 88. 40.]\n",
      "[68. 12. 88. 40.]\n",
      "[-76.  20.  88.  40.]\n",
      "[-68.  20.  88.  40.]\n",
      "[-60.  20.  88.  40.]\n",
      "[-52.  20.  88.  40.]\n",
      "[-44.  20.  88.  40.]\n",
      "[-36.  20.  88.  40.]\n",
      "[-28.  20.  88.  40.]\n",
      "[-20.  20.  88.  40.]\n",
      "[-12.  20.  88.  40.]\n",
      "[-4. 20. 88. 40.]\n",
      "[ 4. 20. 88. 40.]\n",
      "[12. 20. 88. 40.]\n",
      "[20. 20. 88. 40.]\n",
      "[28. 20. 88. 40.]\n",
      "[36. 20. 88. 40.]\n",
      "[44. 20. 88. 40.]\n",
      "[52. 20. 88. 40.]\n",
      "[60. 20. 88. 40.]\n",
      "[68. 20. 88. 40.]\n",
      "[-76.  28.  88.  40.]\n",
      "[-68.  28.  88.  40.]\n",
      "[-60.  28.  88.  40.]\n",
      "[-52.  28.  88.  40.]\n",
      "[-44.  28.  88.  40.]\n",
      "[-36.  28.  88.  40.]\n",
      "[-28.  28.  88.  40.]\n",
      "[-20.  28.  88.  40.]\n",
      "[-12.  28.  88.  40.]\n",
      "[-4. 28. 88. 40.]\n",
      "[ 4. 28. 88. 40.]\n",
      "[12. 28. 88. 40.]\n",
      "[20. 28. 88. 40.]\n",
      "[28. 28. 88. 40.]\n",
      "[36. 28. 88. 40.]\n",
      "[44. 28. 88. 40.]\n",
      "[52. 28. 88. 40.]\n",
      "[60. 28. 88. 40.]\n",
      "[68. 28. 88. 40.]\n",
      "[-76.  36.  88.  40.]\n",
      "[-68.  36.  88.  40.]\n",
      "[-60.  36.  88.  40.]\n",
      "[-52.  36.  88.  40.]\n",
      "[-44.  36.  88.  40.]\n",
      "[-36.  36.  88.  40.]\n",
      "[-28.  36.  88.  40.]\n",
      "[-20.  36.  88.  40.]\n",
      "[-12.  36.  88.  40.]\n",
      "[-4. 36. 88. 40.]\n",
      "[ 4. 36. 88. 40.]\n",
      "[12. 36. 88. 40.]\n",
      "[20. 36. 88. 40.]\n",
      "[28. 36. 88. 40.]\n",
      "[36. 36. 88. 40.]\n",
      "[44. 36. 88. 40.]\n",
      "[52. 36. 88. 40.]\n",
      "[60. 36. 88. 40.]\n",
      "[68. 36. 88. 40.]\n",
      "[-76.  44.  88.  40.]\n",
      "[-68.  44.  88.  40.]\n",
      "[-60.  44.  88.  40.]\n",
      "[-52.  44.  88.  40.]\n",
      "[-44.  44.  88.  40.]\n",
      "[-36.  44.  88.  40.]\n",
      "[-28.  44.  88.  40.]\n",
      "[-20.  44.  88.  40.]\n",
      "[-12.  44.  88.  40.]\n",
      "[-4. 44. 88. 40.]\n",
      "[ 4. 44. 88. 40.]\n",
      "[12. 44. 88. 40.]\n",
      "[20. 44. 88. 40.]\n",
      "[28. 44. 88. 40.]\n",
      "[36. 44. 88. 40.]\n",
      "[44. 44. 88. 40.]\n",
      "[52. 44. 88. 40.]\n",
      "[60. 44. 88. 40.]\n",
      "[68. 44. 88. 40.]\n",
      "[-76.  52.  88.  40.]\n",
      "[-68.  52.  88.  40.]\n",
      "[-60.  52.  88.  40.]\n",
      "[-52.  52.  88.  40.]\n",
      "[-44.  52.  88.  40.]\n",
      "[-36.  52.  88.  40.]\n",
      "[-28.  52.  88.  40.]\n",
      "[-20.  52.  88.  40.]\n",
      "[-12.  52.  88.  40.]\n",
      "[-4. 52. 88. 40.]\n",
      "[ 4. 52. 88. 40.]\n",
      "[12. 52. 88. 40.]\n",
      "[20. 52. 88. 40.]\n",
      "[28. 52. 88. 40.]\n",
      "[36. 52. 88. 40.]\n",
      "[44. 52. 88. 40.]\n",
      "[52. 52. 88. 40.]\n",
      "[60. 52. 88. 40.]\n",
      "[68. 52. 88. 40.]\n",
      "[-76.  60.  88.  40.]\n",
      "[-68.  60.  88.  40.]\n",
      "[-60.  60.  88.  40.]\n",
      "[-52.  60.  88.  40.]\n",
      "[-44.  60.  88.  40.]\n",
      "[-36.  60.  88.  40.]\n",
      "[-28.  60.  88.  40.]\n",
      "[-20.  60.  88.  40.]\n",
      "[-12.  60.  88.  40.]\n",
      "[-4. 60. 88. 40.]\n",
      "[ 4. 60. 88. 40.]\n",
      "[12. 60. 88. 40.]\n",
      "[20. 60. 88. 40.]\n",
      "[28. 60. 88. 40.]\n",
      "[36. 60. 88. 40.]\n",
      "[44. 60. 88. 40.]\n",
      "[52. 60. 88. 40.]\n",
      "[60. 60. 88. 40.]\n",
      "[68. 60. 88. 40.]\n",
      "[-76.  68.  88.  40.]\n",
      "[-68.  68.  88.  40.]\n",
      "[-60.  68.  88.  40.]\n",
      "[-52.  68.  88.  40.]\n",
      "[-44.  68.  88.  40.]\n",
      "[-36.  68.  88.  40.]\n",
      "[-28.  68.  88.  40.]\n",
      "[-20.  68.  88.  40.]\n",
      "[-12.  68.  88.  40.]\n",
      "[-4. 68. 88. 40.]\n",
      "[ 4. 68. 88. 40.]\n",
      "[12. 68. 88. 40.]\n",
      "[20. 68. 88. 40.]\n",
      "[28. 68. 88. 40.]\n",
      "[36. 68. 88. 40.]\n",
      "[44. 68. 88. 40.]\n",
      "[52. 68. 88. 40.]\n",
      "[60. 68. 88. 40.]\n",
      "[68. 68. 88. 40.]\n",
      "[-76. -76.  64.  64.]\n",
      "[-68. -76.  64.  64.]\n",
      "[-60. -76.  64.  64.]\n",
      "[-52. -76.  64.  64.]\n",
      "[-44. -76.  64.  64.]\n",
      "[-36. -76.  64.  64.]\n",
      "[-28. -76.  64.  64.]\n",
      "[-20. -76.  64.  64.]\n",
      "[-12. -76.  64.  64.]\n",
      "[ -4. -76.  64.  64.]\n",
      "[  4. -76.  64.  64.]\n",
      "[ 12. -76.  64.  64.]\n",
      "[ 20. -76.  64.  64.]\n",
      "[ 28. -76.  64.  64.]\n",
      "[ 36. -76.  64.  64.]\n",
      "[ 44. -76.  64.  64.]\n",
      "[ 52. -76.  64.  64.]\n",
      "[ 60. -76.  64.  64.]\n",
      "[ 68. -76.  64.  64.]\n",
      "[-76. -68.  64.  64.]\n",
      "[-68. -68.  64.  64.]\n",
      "[-60. -68.  64.  64.]\n",
      "[-52. -68.  64.  64.]\n",
      "[-44. -68.  64.  64.]\n",
      "[-36. -68.  64.  64.]\n",
      "[-28. -68.  64.  64.]\n",
      "[-20. -68.  64.  64.]\n",
      "[-12. -68.  64.  64.]\n",
      "[ -4. -68.  64.  64.]\n",
      "[  4. -68.  64.  64.]\n",
      "[ 12. -68.  64.  64.]\n",
      "[ 20. -68.  64.  64.]\n",
      "[ 28. -68.  64.  64.]\n",
      "[ 36. -68.  64.  64.]\n",
      "[ 44. -68.  64.  64.]\n",
      "[ 52. -68.  64.  64.]\n",
      "[ 60. -68.  64.  64.]\n",
      "[ 68. -68.  64.  64.]\n",
      "[-76. -60.  64.  64.]\n",
      "[-68. -60.  64.  64.]\n",
      "[-60. -60.  64.  64.]\n",
      "[-52. -60.  64.  64.]\n",
      "[-44. -60.  64.  64.]\n",
      "[-36. -60.  64.  64.]\n",
      "[-28. -60.  64.  64.]\n",
      "[-20. -60.  64.  64.]\n",
      "[-12. -60.  64.  64.]\n",
      "[ -4. -60.  64.  64.]\n",
      "[  4. -60.  64.  64.]\n",
      "[ 12. -60.  64.  64.]\n",
      "[ 20. -60.  64.  64.]\n",
      "[ 28. -60.  64.  64.]\n",
      "[ 36. -60.  64.  64.]\n",
      "[ 44. -60.  64.  64.]\n",
      "[ 52. -60.  64.  64.]\n",
      "[ 60. -60.  64.  64.]\n",
      "[ 68. -60.  64.  64.]\n",
      "[-76. -52.  64.  64.]\n",
      "[-68. -52.  64.  64.]\n",
      "[-60. -52.  64.  64.]\n",
      "[-52. -52.  64.  64.]\n",
      "[-44. -52.  64.  64.]\n",
      "[-36. -52.  64.  64.]\n",
      "[-28. -52.  64.  64.]\n",
      "[-20. -52.  64.  64.]\n",
      "[-12. -52.  64.  64.]\n",
      "[ -4. -52.  64.  64.]\n",
      "[  4. -52.  64.  64.]\n",
      "[ 12. -52.  64.  64.]\n",
      "[ 20. -52.  64.  64.]\n",
      "[ 28. -52.  64.  64.]\n",
      "[ 36. -52.  64.  64.]\n",
      "[ 44. -52.  64.  64.]\n",
      "[ 52. -52.  64.  64.]\n",
      "[ 60. -52.  64.  64.]\n",
      "[ 68. -52.  64.  64.]\n",
      "[-76. -44.  64.  64.]\n",
      "[-68. -44.  64.  64.]\n",
      "[-60. -44.  64.  64.]\n",
      "[-52. -44.  64.  64.]\n",
      "[-44. -44.  64.  64.]\n",
      "[-36. -44.  64.  64.]\n",
      "[-28. -44.  64.  64.]\n",
      "[-20. -44.  64.  64.]\n",
      "[-12. -44.  64.  64.]\n",
      "[ -4. -44.  64.  64.]\n",
      "[  4. -44.  64.  64.]\n",
      "[ 12. -44.  64.  64.]\n",
      "[ 20. -44.  64.  64.]\n",
      "[ 28. -44.  64.  64.]\n",
      "[ 36. -44.  64.  64.]\n",
      "[ 44. -44.  64.  64.]\n",
      "[ 52. -44.  64.  64.]\n",
      "[ 60. -44.  64.  64.]\n",
      "[ 68. -44.  64.  64.]\n",
      "[-76. -36.  64.  64.]\n",
      "[-68. -36.  64.  64.]\n",
      "[-60. -36.  64.  64.]\n",
      "[-52. -36.  64.  64.]\n",
      "[-44. -36.  64.  64.]\n",
      "[-36. -36.  64.  64.]\n",
      "[-28. -36.  64.  64.]\n",
      "[-20. -36.  64.  64.]\n",
      "[-12. -36.  64.  64.]\n",
      "[ -4. -36.  64.  64.]\n",
      "[  4. -36.  64.  64.]\n",
      "[ 12. -36.  64.  64.]\n",
      "[ 20. -36.  64.  64.]\n",
      "[ 28. -36.  64.  64.]\n",
      "[ 36. -36.  64.  64.]\n",
      "[ 44. -36.  64.  64.]\n",
      "[ 52. -36.  64.  64.]\n",
      "[ 60. -36.  64.  64.]\n",
      "[ 68. -36.  64.  64.]\n",
      "[-76. -28.  64.  64.]\n",
      "[-68. -28.  64.  64.]\n",
      "[-60. -28.  64.  64.]\n",
      "[-52. -28.  64.  64.]\n",
      "[-44. -28.  64.  64.]\n",
      "[-36. -28.  64.  64.]\n",
      "[-28. -28.  64.  64.]\n",
      "[-20. -28.  64.  64.]\n",
      "[-12. -28.  64.  64.]\n",
      "[ -4. -28.  64.  64.]\n",
      "[  4. -28.  64.  64.]\n",
      "[ 12. -28.  64.  64.]\n",
      "[ 20. -28.  64.  64.]\n",
      "[ 28. -28.  64.  64.]\n",
      "[ 36. -28.  64.  64.]\n",
      "[ 44. -28.  64.  64.]\n",
      "[ 52. -28.  64.  64.]\n",
      "[ 60. -28.  64.  64.]\n",
      "[ 68. -28.  64.  64.]\n",
      "[-76. -20.  64.  64.]\n",
      "[-68. -20.  64.  64.]\n",
      "[-60. -20.  64.  64.]\n",
      "[-52. -20.  64.  64.]\n",
      "[-44. -20.  64.  64.]\n",
      "[-36. -20.  64.  64.]\n",
      "[-28. -20.  64.  64.]\n",
      "[-20. -20.  64.  64.]\n",
      "[-12. -20.  64.  64.]\n",
      "[ -4. -20.  64.  64.]\n",
      "[  4. -20.  64.  64.]\n",
      "[ 12. -20.  64.  64.]\n",
      "[ 20. -20.  64.  64.]\n",
      "[ 28. -20.  64.  64.]\n",
      "[ 36. -20.  64.  64.]\n",
      "[ 44. -20.  64.  64.]\n",
      "[ 52. -20.  64.  64.]\n",
      "[ 60. -20.  64.  64.]\n",
      "[ 68. -20.  64.  64.]\n",
      "[-76. -12.  64.  64.]\n",
      "[-68. -12.  64.  64.]\n",
      "[-60. -12.  64.  64.]\n",
      "[-52. -12.  64.  64.]\n",
      "[-44. -12.  64.  64.]\n",
      "[-36. -12.  64.  64.]\n",
      "[-28. -12.  64.  64.]\n",
      "[-20. -12.  64.  64.]\n",
      "[-12. -12.  64.  64.]\n",
      "[ -4. -12.  64.  64.]\n",
      "[  4. -12.  64.  64.]\n",
      "[ 12. -12.  64.  64.]\n",
      "[ 20. -12.  64.  64.]\n",
      "[ 28. -12.  64.  64.]\n",
      "[ 36. -12.  64.  64.]\n",
      "[ 44. -12.  64.  64.]\n",
      "[ 52. -12.  64.  64.]\n",
      "[ 60. -12.  64.  64.]\n",
      "[ 68. -12.  64.  64.]\n",
      "[-76.  -4.  64.  64.]\n",
      "[-68.  -4.  64.  64.]\n",
      "[-60.  -4.  64.  64.]\n",
      "[-52.  -4.  64.  64.]\n",
      "[-44.  -4.  64.  64.]\n",
      "[-36.  -4.  64.  64.]\n",
      "[-28.  -4.  64.  64.]\n",
      "[-20.  -4.  64.  64.]\n",
      "[-12.  -4.  64.  64.]\n",
      "[-4. -4. 64. 64.]\n",
      "[ 4. -4. 64. 64.]\n",
      "[12. -4. 64. 64.]\n",
      "[20. -4. 64. 64.]\n",
      "[28. -4. 64. 64.]\n",
      "[36. -4. 64. 64.]\n",
      "[44. -4. 64. 64.]\n",
      "[52. -4. 64. 64.]\n",
      "[60. -4. 64. 64.]\n",
      "[68. -4. 64. 64.]\n",
      "[-76.   4.  64.  64.]\n",
      "[-68.   4.  64.  64.]\n",
      "[-60.   4.  64.  64.]\n",
      "[-52.   4.  64.  64.]\n",
      "[-44.   4.  64.  64.]\n",
      "[-36.   4.  64.  64.]\n",
      "[-28.   4.  64.  64.]\n",
      "[-20.   4.  64.  64.]\n",
      "[-12.   4.  64.  64.]\n",
      "[-4.  4. 64. 64.]\n",
      "[ 4.  4. 64. 64.]\n",
      "[12.  4. 64. 64.]\n",
      "[20.  4. 64. 64.]\n",
      "[28.  4. 64. 64.]\n",
      "[36.  4. 64. 64.]\n",
      "[44.  4. 64. 64.]\n",
      "[52.  4. 64. 64.]\n",
      "[60.  4. 64. 64.]\n",
      "[68.  4. 64. 64.]\n",
      "[-76.  12.  64.  64.]\n",
      "[-68.  12.  64.  64.]\n",
      "[-60.  12.  64.  64.]\n",
      "[-52.  12.  64.  64.]\n",
      "[-44.  12.  64.  64.]\n",
      "[-36.  12.  64.  64.]\n",
      "[-28.  12.  64.  64.]\n",
      "[-20.  12.  64.  64.]\n",
      "[-12.  12.  64.  64.]\n",
      "[-4. 12. 64. 64.]\n",
      "[ 4. 12. 64. 64.]\n",
      "[12. 12. 64. 64.]\n",
      "[20. 12. 64. 64.]\n",
      "[28. 12. 64. 64.]\n",
      "[36. 12. 64. 64.]\n",
      "[44. 12. 64. 64.]\n",
      "[52. 12. 64. 64.]\n",
      "[60. 12. 64. 64.]\n",
      "[68. 12. 64. 64.]\n",
      "[-76.  20.  64.  64.]\n",
      "[-68.  20.  64.  64.]\n",
      "[-60.  20.  64.  64.]\n",
      "[-52.  20.  64.  64.]\n",
      "[-44.  20.  64.  64.]\n",
      "[-36.  20.  64.  64.]\n",
      "[-28.  20.  64.  64.]\n",
      "[-20.  20.  64.  64.]\n",
      "[-12.  20.  64.  64.]\n",
      "[-4. 20. 64. 64.]\n",
      "[ 4. 20. 64. 64.]\n",
      "[12. 20. 64. 64.]\n",
      "[20. 20. 64. 64.]\n",
      "[28. 20. 64. 64.]\n",
      "[36. 20. 64. 64.]\n",
      "[44. 20. 64. 64.]\n",
      "[52. 20. 64. 64.]\n",
      "[60. 20. 64. 64.]\n",
      "[68. 20. 64. 64.]\n",
      "[-76.  28.  64.  64.]\n",
      "[-68.  28.  64.  64.]\n",
      "[-60.  28.  64.  64.]\n",
      "[-52.  28.  64.  64.]\n",
      "[-44.  28.  64.  64.]\n",
      "[-36.  28.  64.  64.]\n",
      "[-28.  28.  64.  64.]\n",
      "[-20.  28.  64.  64.]\n",
      "[-12.  28.  64.  64.]\n",
      "[-4. 28. 64. 64.]\n",
      "[ 4. 28. 64. 64.]\n",
      "[12. 28. 64. 64.]\n",
      "[20. 28. 64. 64.]\n",
      "[28. 28. 64. 64.]\n",
      "[36. 28. 64. 64.]\n",
      "[44. 28. 64. 64.]\n",
      "[52. 28. 64. 64.]\n",
      "[60. 28. 64. 64.]\n",
      "[68. 28. 64. 64.]\n",
      "[-76.  36.  64.  64.]\n",
      "[-68.  36.  64.  64.]\n",
      "[-60.  36.  64.  64.]\n",
      "[-52.  36.  64.  64.]\n",
      "[-44.  36.  64.  64.]\n",
      "[-36.  36.  64.  64.]\n",
      "[-28.  36.  64.  64.]\n",
      "[-20.  36.  64.  64.]\n",
      "[-12.  36.  64.  64.]\n",
      "[-4. 36. 64. 64.]\n",
      "[ 4. 36. 64. 64.]\n",
      "[12. 36. 64. 64.]\n",
      "[20. 36. 64. 64.]\n",
      "[28. 36. 64. 64.]\n",
      "[36. 36. 64. 64.]\n",
      "[44. 36. 64. 64.]\n",
      "[52. 36. 64. 64.]\n",
      "[60. 36. 64. 64.]\n",
      "[68. 36. 64. 64.]\n",
      "[-76.  44.  64.  64.]\n",
      "[-68.  44.  64.  64.]\n",
      "[-60.  44.  64.  64.]\n",
      "[-52.  44.  64.  64.]\n",
      "[-44.  44.  64.  64.]\n",
      "[-36.  44.  64.  64.]\n",
      "[-28.  44.  64.  64.]\n",
      "[-20.  44.  64.  64.]\n",
      "[-12.  44.  64.  64.]\n",
      "[-4. 44. 64. 64.]\n",
      "[ 4. 44. 64. 64.]\n",
      "[12. 44. 64. 64.]\n",
      "[20. 44. 64. 64.]\n",
      "[28. 44. 64. 64.]\n",
      "[36. 44. 64. 64.]\n",
      "[44. 44. 64. 64.]\n",
      "[52. 44. 64. 64.]\n",
      "[60. 44. 64. 64.]\n",
      "[68. 44. 64. 64.]\n",
      "[-76.  52.  64.  64.]\n",
      "[-68.  52.  64.  64.]\n",
      "[-60.  52.  64.  64.]\n",
      "[-52.  52.  64.  64.]\n",
      "[-44.  52.  64.  64.]\n",
      "[-36.  52.  64.  64.]\n",
      "[-28.  52.  64.  64.]\n",
      "[-20.  52.  64.  64.]\n",
      "[-12.  52.  64.  64.]\n",
      "[-4. 52. 64. 64.]\n",
      "[ 4. 52. 64. 64.]\n",
      "[12. 52. 64. 64.]\n",
      "[20. 52. 64. 64.]\n",
      "[28. 52. 64. 64.]\n",
      "[36. 52. 64. 64.]\n",
      "[44. 52. 64. 64.]\n",
      "[52. 52. 64. 64.]\n",
      "[60. 52. 64. 64.]\n",
      "[68. 52. 64. 64.]\n",
      "[-76.  60.  64.  64.]\n",
      "[-68.  60.  64.  64.]\n",
      "[-60.  60.  64.  64.]\n",
      "[-52.  60.  64.  64.]\n",
      "[-44.  60.  64.  64.]\n",
      "[-36.  60.  64.  64.]\n",
      "[-28.  60.  64.  64.]\n",
      "[-20.  60.  64.  64.]\n",
      "[-12.  60.  64.  64.]\n",
      "[-4. 60. 64. 64.]\n",
      "[ 4. 60. 64. 64.]\n",
      "[12. 60. 64. 64.]\n",
      "[20. 60. 64. 64.]\n",
      "[28. 60. 64. 64.]\n",
      "[36. 60. 64. 64.]\n",
      "[44. 60. 64. 64.]\n",
      "[52. 60. 64. 64.]\n",
      "[60. 60. 64. 64.]\n",
      "[68. 60. 64. 64.]\n",
      "[-76.  68.  64.  64.]\n",
      "[-68.  68.  64.  64.]\n",
      "[-60.  68.  64.  64.]\n",
      "[-52.  68.  64.  64.]\n",
      "[-44.  68.  64.  64.]\n",
      "[-36.  68.  64.  64.]\n",
      "[-28.  68.  64.  64.]\n",
      "[-20.  68.  64.  64.]\n",
      "[-12.  68.  64.  64.]\n",
      "[-4. 68. 64. 64.]\n",
      "[ 4. 68. 64. 64.]\n",
      "[12. 68. 64. 64.]\n",
      "[20. 68. 64. 64.]\n",
      "[28. 68. 64. 64.]\n",
      "[36. 68. 64. 64.]\n",
      "[44. 68. 64. 64.]\n",
      "[52. 68. 64. 64.]\n",
      "[60. 68. 64. 64.]\n",
      "[68. 68. 64. 64.]\n",
      "[-76. -76.  40.  80.]\n",
      "[-68. -76.  40.  80.]\n",
      "[-60. -76.  40.  80.]\n",
      "[-52. -76.  40.  80.]\n",
      "[-44. -76.  40.  80.]\n",
      "[-36. -76.  40.  80.]\n",
      "[-28. -76.  40.  80.]\n",
      "[-20. -76.  40.  80.]\n",
      "[-12. -76.  40.  80.]\n",
      "[ -4. -76.  40.  80.]\n",
      "[  4. -76.  40.  80.]\n",
      "[ 12. -76.  40.  80.]\n",
      "[ 20. -76.  40.  80.]\n",
      "[ 28. -76.  40.  80.]\n",
      "[ 36. -76.  40.  80.]\n",
      "[ 44. -76.  40.  80.]\n",
      "[ 52. -76.  40.  80.]\n",
      "[ 60. -76.  40.  80.]\n",
      "[ 68. -76.  40.  80.]\n",
      "[-76. -68.  40.  80.]\n",
      "[-68. -68.  40.  80.]\n",
      "[-60. -68.  40.  80.]\n",
      "[-52. -68.  40.  80.]\n",
      "[-44. -68.  40.  80.]\n",
      "[-36. -68.  40.  80.]\n",
      "[-28. -68.  40.  80.]\n",
      "[-20. -68.  40.  80.]\n",
      "[-12. -68.  40.  80.]\n",
      "[ -4. -68.  40.  80.]\n",
      "[  4. -68.  40.  80.]\n",
      "[ 12. -68.  40.  80.]\n",
      "[ 20. -68.  40.  80.]\n",
      "[ 28. -68.  40.  80.]\n",
      "[ 36. -68.  40.  80.]\n",
      "[ 44. -68.  40.  80.]\n",
      "[ 52. -68.  40.  80.]\n",
      "[ 60. -68.  40.  80.]\n",
      "[ 68. -68.  40.  80.]\n",
      "[-76. -60.  40.  80.]\n",
      "[-68. -60.  40.  80.]\n",
      "[-60. -60.  40.  80.]\n",
      "[-52. -60.  40.  80.]\n",
      "[-44. -60.  40.  80.]\n",
      "[-36. -60.  40.  80.]\n",
      "[-28. -60.  40.  80.]\n",
      "[-20. -60.  40.  80.]\n",
      "[-12. -60.  40.  80.]\n",
      "[ -4. -60.  40.  80.]\n",
      "[  4. -60.  40.  80.]\n",
      "[ 12. -60.  40.  80.]\n",
      "[ 20. -60.  40.  80.]\n",
      "[ 28. -60.  40.  80.]\n",
      "[ 36. -60.  40.  80.]\n",
      "[ 44. -60.  40.  80.]\n",
      "[ 52. -60.  40.  80.]\n",
      "[ 60. -60.  40.  80.]\n",
      "[ 68. -60.  40.  80.]\n",
      "[-76. -52.  40.  80.]\n",
      "[-68. -52.  40.  80.]\n",
      "[-60. -52.  40.  80.]\n",
      "[-52. -52.  40.  80.]\n",
      "[-44. -52.  40.  80.]\n",
      "[-36. -52.  40.  80.]\n",
      "[-28. -52.  40.  80.]\n",
      "[-20. -52.  40.  80.]\n",
      "[-12. -52.  40.  80.]\n",
      "[ -4. -52.  40.  80.]\n",
      "[  4. -52.  40.  80.]\n",
      "[ 12. -52.  40.  80.]\n",
      "[ 20. -52.  40.  80.]\n",
      "[ 28. -52.  40.  80.]\n",
      "[ 36. -52.  40.  80.]\n",
      "[ 44. -52.  40.  80.]\n",
      "[ 52. -52.  40.  80.]\n",
      "[ 60. -52.  40.  80.]\n",
      "[ 68. -52.  40.  80.]\n",
      "[-76. -44.  40.  80.]\n",
      "[-68. -44.  40.  80.]\n",
      "[-60. -44.  40.  80.]\n",
      "[-52. -44.  40.  80.]\n",
      "[-44. -44.  40.  80.]\n",
      "[-36. -44.  40.  80.]\n",
      "[-28. -44.  40.  80.]\n",
      "[-20. -44.  40.  80.]\n",
      "[-12. -44.  40.  80.]\n",
      "[ -4. -44.  40.  80.]\n",
      "[  4. -44.  40.  80.]\n",
      "[ 12. -44.  40.  80.]\n",
      "[ 20. -44.  40.  80.]\n",
      "[ 28. -44.  40.  80.]\n",
      "[ 36. -44.  40.  80.]\n",
      "[ 44. -44.  40.  80.]\n",
      "[ 52. -44.  40.  80.]\n",
      "[ 60. -44.  40.  80.]\n",
      "[ 68. -44.  40.  80.]\n",
      "[-76. -36.  40.  80.]\n",
      "[-68. -36.  40.  80.]\n",
      "[-60. -36.  40.  80.]\n",
      "[-52. -36.  40.  80.]\n",
      "[-44. -36.  40.  80.]\n",
      "[-36. -36.  40.  80.]\n",
      "[-28. -36.  40.  80.]\n",
      "[-20. -36.  40.  80.]\n",
      "[-12. -36.  40.  80.]\n",
      "[ -4. -36.  40.  80.]\n",
      "[  4. -36.  40.  80.]\n",
      "[ 12. -36.  40.  80.]\n",
      "[ 20. -36.  40.  80.]\n",
      "[ 28. -36.  40.  80.]\n",
      "[ 36. -36.  40.  80.]\n",
      "[ 44. -36.  40.  80.]\n",
      "[ 52. -36.  40.  80.]\n",
      "[ 60. -36.  40.  80.]\n",
      "[ 68. -36.  40.  80.]\n",
      "[-76. -28.  40.  80.]\n",
      "[-68. -28.  40.  80.]\n",
      "[-60. -28.  40.  80.]\n",
      "[-52. -28.  40.  80.]\n",
      "[-44. -28.  40.  80.]\n",
      "[-36. -28.  40.  80.]\n",
      "[-28. -28.  40.  80.]\n",
      "[-20. -28.  40.  80.]\n",
      "[-12. -28.  40.  80.]\n",
      "[ -4. -28.  40.  80.]\n",
      "[  4. -28.  40.  80.]\n",
      "[ 12. -28.  40.  80.]\n",
      "[ 20. -28.  40.  80.]\n",
      "[ 28. -28.  40.  80.]\n",
      "[ 36. -28.  40.  80.]\n",
      "[ 44. -28.  40.  80.]\n",
      "[ 52. -28.  40.  80.]\n",
      "[ 60. -28.  40.  80.]\n",
      "[ 68. -28.  40.  80.]\n",
      "[-76. -20.  40.  80.]\n",
      "[-68. -20.  40.  80.]\n",
      "[-60. -20.  40.  80.]\n",
      "[-52. -20.  40.  80.]\n",
      "[-44. -20.  40.  80.]\n",
      "[-36. -20.  40.  80.]\n",
      "[-28. -20.  40.  80.]\n",
      "[-20. -20.  40.  80.]\n",
      "[-12. -20.  40.  80.]\n",
      "[ -4. -20.  40.  80.]\n",
      "[  4. -20.  40.  80.]\n",
      "[ 12. -20.  40.  80.]\n",
      "[ 20. -20.  40.  80.]\n",
      "[ 28. -20.  40.  80.]\n",
      "[ 36. -20.  40.  80.]\n",
      "[ 44. -20.  40.  80.]\n",
      "[ 52. -20.  40.  80.]\n",
      "[ 60. -20.  40.  80.]\n",
      "[ 68. -20.  40.  80.]\n",
      "[-76. -12.  40.  80.]\n",
      "[-68. -12.  40.  80.]\n",
      "[-60. -12.  40.  80.]\n",
      "[-52. -12.  40.  80.]\n",
      "[-44. -12.  40.  80.]\n",
      "[-36. -12.  40.  80.]\n",
      "[-28. -12.  40.  80.]\n",
      "[-20. -12.  40.  80.]\n",
      "[-12. -12.  40.  80.]\n",
      "[ -4. -12.  40.  80.]\n",
      "[  4. -12.  40.  80.]\n",
      "[ 12. -12.  40.  80.]\n",
      "[ 20. -12.  40.  80.]\n",
      "[ 28. -12.  40.  80.]\n",
      "[ 36. -12.  40.  80.]\n",
      "[ 44. -12.  40.  80.]\n",
      "[ 52. -12.  40.  80.]\n",
      "[ 60. -12.  40.  80.]\n",
      "[ 68. -12.  40.  80.]\n",
      "[-76.  -4.  40.  80.]\n",
      "[-68.  -4.  40.  80.]\n",
      "[-60.  -4.  40.  80.]\n",
      "[-52.  -4.  40.  80.]\n",
      "[-44.  -4.  40.  80.]\n",
      "[-36.  -4.  40.  80.]\n",
      "[-28.  -4.  40.  80.]\n",
      "[-20.  -4.  40.  80.]\n",
      "[-12.  -4.  40.  80.]\n",
      "[-4. -4. 40. 80.]\n",
      "[ 4. -4. 40. 80.]\n",
      "[12. -4. 40. 80.]\n",
      "[20. -4. 40. 80.]\n",
      "[28. -4. 40. 80.]\n",
      "[36. -4. 40. 80.]\n",
      "[44. -4. 40. 80.]\n",
      "[52. -4. 40. 80.]\n",
      "[60. -4. 40. 80.]\n",
      "[68. -4. 40. 80.]\n",
      "[-76.   4.  40.  80.]\n",
      "[-68.   4.  40.  80.]\n",
      "[-60.   4.  40.  80.]\n",
      "[-52.   4.  40.  80.]\n",
      "[-44.   4.  40.  80.]\n",
      "[-36.   4.  40.  80.]\n",
      "[-28.   4.  40.  80.]\n",
      "[-20.   4.  40.  80.]\n",
      "[-12.   4.  40.  80.]\n",
      "[-4.  4. 40. 80.]\n",
      "[ 4.  4. 40. 80.]\n",
      "[12.  4. 40. 80.]\n",
      "[20.  4. 40. 80.]\n",
      "[28.  4. 40. 80.]\n",
      "[36.  4. 40. 80.]\n",
      "[44.  4. 40. 80.]\n",
      "[52.  4. 40. 80.]\n",
      "[60.  4. 40. 80.]\n",
      "[68.  4. 40. 80.]\n",
      "[-76.  12.  40.  80.]\n",
      "[-68.  12.  40.  80.]\n",
      "[-60.  12.  40.  80.]\n",
      "[-52.  12.  40.  80.]\n",
      "[-44.  12.  40.  80.]\n",
      "[-36.  12.  40.  80.]\n",
      "[-28.  12.  40.  80.]\n",
      "[-20.  12.  40.  80.]\n",
      "[-12.  12.  40.  80.]\n",
      "[-4. 12. 40. 80.]\n",
      "[ 4. 12. 40. 80.]\n",
      "[12. 12. 40. 80.]\n",
      "[20. 12. 40. 80.]\n",
      "[28. 12. 40. 80.]\n",
      "[36. 12. 40. 80.]\n",
      "[44. 12. 40. 80.]\n",
      "[52. 12. 40. 80.]\n",
      "[60. 12. 40. 80.]\n",
      "[68. 12. 40. 80.]\n",
      "[-76.  20.  40.  80.]\n",
      "[-68.  20.  40.  80.]\n",
      "[-60.  20.  40.  80.]\n",
      "[-52.  20.  40.  80.]\n",
      "[-44.  20.  40.  80.]\n",
      "[-36.  20.  40.  80.]\n",
      "[-28.  20.  40.  80.]\n",
      "[-20.  20.  40.  80.]\n",
      "[-12.  20.  40.  80.]\n",
      "[-4. 20. 40. 80.]\n",
      "[ 4. 20. 40. 80.]\n",
      "[12. 20. 40. 80.]\n",
      "[20. 20. 40. 80.]\n",
      "[28. 20. 40. 80.]\n",
      "[36. 20. 40. 80.]\n",
      "[44. 20. 40. 80.]\n",
      "[52. 20. 40. 80.]\n",
      "[60. 20. 40. 80.]\n",
      "[68. 20. 40. 80.]\n",
      "[-76.  28.  40.  80.]\n",
      "[-68.  28.  40.  80.]\n",
      "[-60.  28.  40.  80.]\n",
      "[-52.  28.  40.  80.]\n",
      "[-44.  28.  40.  80.]\n",
      "[-36.  28.  40.  80.]\n",
      "[-28.  28.  40.  80.]\n",
      "[-20.  28.  40.  80.]\n",
      "[-12.  28.  40.  80.]\n",
      "[-4. 28. 40. 80.]\n",
      "[ 4. 28. 40. 80.]\n",
      "[12. 28. 40. 80.]\n",
      "[20. 28. 40. 80.]\n",
      "[28. 28. 40. 80.]\n",
      "[36. 28. 40. 80.]\n",
      "[44. 28. 40. 80.]\n",
      "[52. 28. 40. 80.]\n",
      "[60. 28. 40. 80.]\n",
      "[68. 28. 40. 80.]\n",
      "[-76.  36.  40.  80.]\n",
      "[-68.  36.  40.  80.]\n",
      "[-60.  36.  40.  80.]\n",
      "[-52.  36.  40.  80.]\n",
      "[-44.  36.  40.  80.]\n",
      "[-36.  36.  40.  80.]\n",
      "[-28.  36.  40.  80.]\n",
      "[-20.  36.  40.  80.]\n",
      "[-12.  36.  40.  80.]\n",
      "[-4. 36. 40. 80.]\n",
      "[ 4. 36. 40. 80.]\n",
      "[12. 36. 40. 80.]\n",
      "[20. 36. 40. 80.]\n",
      "[28. 36. 40. 80.]\n",
      "[36. 36. 40. 80.]\n",
      "[44. 36. 40. 80.]\n",
      "[52. 36. 40. 80.]\n",
      "[60. 36. 40. 80.]\n",
      "[68. 36. 40. 80.]\n",
      "[-76.  44.  40.  80.]\n",
      "[-68.  44.  40.  80.]\n",
      "[-60.  44.  40.  80.]\n",
      "[-52.  44.  40.  80.]\n",
      "[-44.  44.  40.  80.]\n",
      "[-36.  44.  40.  80.]\n",
      "[-28.  44.  40.  80.]\n",
      "[-20.  44.  40.  80.]\n",
      "[-12.  44.  40.  80.]\n",
      "[-4. 44. 40. 80.]\n",
      "[ 4. 44. 40. 80.]\n",
      "[12. 44. 40. 80.]\n",
      "[20. 44. 40. 80.]\n",
      "[28. 44. 40. 80.]\n",
      "[36. 44. 40. 80.]\n",
      "[44. 44. 40. 80.]\n",
      "[52. 44. 40. 80.]\n",
      "[60. 44. 40. 80.]\n",
      "[68. 44. 40. 80.]\n",
      "[-76.  52.  40.  80.]\n",
      "[-68.  52.  40.  80.]\n",
      "[-60.  52.  40.  80.]\n",
      "[-52.  52.  40.  80.]\n",
      "[-44.  52.  40.  80.]\n",
      "[-36.  52.  40.  80.]\n",
      "[-28.  52.  40.  80.]\n",
      "[-20.  52.  40.  80.]\n",
      "[-12.  52.  40.  80.]\n",
      "[-4. 52. 40. 80.]\n",
      "[ 4. 52. 40. 80.]\n",
      "[12. 52. 40. 80.]\n",
      "[20. 52. 40. 80.]\n",
      "[28. 52. 40. 80.]\n",
      "[36. 52. 40. 80.]\n",
      "[44. 52. 40. 80.]\n",
      "[52. 52. 40. 80.]\n",
      "[60. 52. 40. 80.]\n",
      "[68. 52. 40. 80.]\n",
      "[-76.  60.  40.  80.]\n",
      "[-68.  60.  40.  80.]\n",
      "[-60.  60.  40.  80.]\n",
      "[-52.  60.  40.  80.]\n",
      "[-44.  60.  40.  80.]\n",
      "[-36.  60.  40.  80.]\n",
      "[-28.  60.  40.  80.]\n",
      "[-20.  60.  40.  80.]\n",
      "[-12.  60.  40.  80.]\n",
      "[-4. 60. 40. 80.]\n",
      "[ 4. 60. 40. 80.]\n",
      "[12. 60. 40. 80.]\n",
      "[20. 60. 40. 80.]\n",
      "[28. 60. 40. 80.]\n",
      "[36. 60. 40. 80.]\n",
      "[44. 60. 40. 80.]\n",
      "[52. 60. 40. 80.]\n",
      "[60. 60. 40. 80.]\n",
      "[68. 60. 40. 80.]\n",
      "[-76.  68.  40.  80.]\n",
      "[-68.  68.  40.  80.]\n",
      "[-60.  68.  40.  80.]\n",
      "[-52.  68.  40.  80.]\n",
      "[-44.  68.  40.  80.]\n",
      "[-36.  68.  40.  80.]\n",
      "[-28.  68.  40.  80.]\n",
      "[-20.  68.  40.  80.]\n",
      "[-12.  68.  40.  80.]\n",
      "[-4. 68. 40. 80.]\n",
      "[ 4. 68. 40. 80.]\n",
      "[12. 68. 40. 80.]\n",
      "[20. 68. 40. 80.]\n",
      "[28. 68. 40. 80.]\n",
      "[36. 68. 40. 80.]\n",
      "[44. 68. 40. 80.]\n",
      "[52. 68. 40. 80.]\n",
      "[60. 68. 40. 80.]\n",
      "[68. 68. 40. 80.]\n",
      "[-76. -76.  32.  96.]\n",
      "[-68. -76.  32.  96.]\n",
      "[-60. -76.  32.  96.]\n",
      "[-52. -76.  32.  96.]\n",
      "[-44. -76.  32.  96.]\n",
      "[-36. -76.  32.  96.]\n",
      "[-28. -76.  32.  96.]\n",
      "[-20. -76.  32.  96.]\n",
      "[-12. -76.  32.  96.]\n",
      "[ -4. -76.  32.  96.]\n",
      "[  4. -76.  32.  96.]\n",
      "[ 12. -76.  32.  96.]\n",
      "[ 20. -76.  32.  96.]\n",
      "[ 28. -76.  32.  96.]\n",
      "[ 36. -76.  32.  96.]\n",
      "[ 44. -76.  32.  96.]\n",
      "[ 52. -76.  32.  96.]\n",
      "[ 60. -76.  32.  96.]\n",
      "[ 68. -76.  32.  96.]\n",
      "[-76. -68.  32.  96.]\n",
      "[-68. -68.  32.  96.]\n",
      "[-60. -68.  32.  96.]\n",
      "[-52. -68.  32.  96.]\n",
      "[-44. -68.  32.  96.]\n",
      "[-36. -68.  32.  96.]\n",
      "[-28. -68.  32.  96.]\n",
      "[-20. -68.  32.  96.]\n",
      "[-12. -68.  32.  96.]\n",
      "[ -4. -68.  32.  96.]\n",
      "[  4. -68.  32.  96.]\n",
      "[ 12. -68.  32.  96.]\n",
      "[ 20. -68.  32.  96.]\n",
      "[ 28. -68.  32.  96.]\n",
      "[ 36. -68.  32.  96.]\n",
      "[ 44. -68.  32.  96.]\n",
      "[ 52. -68.  32.  96.]\n",
      "[ 60. -68.  32.  96.]\n",
      "[ 68. -68.  32.  96.]\n",
      "[-76. -60.  32.  96.]\n",
      "[-68. -60.  32.  96.]\n",
      "[-60. -60.  32.  96.]\n",
      "[-52. -60.  32.  96.]\n",
      "[-44. -60.  32.  96.]\n",
      "[-36. -60.  32.  96.]\n",
      "[-28. -60.  32.  96.]\n",
      "[-20. -60.  32.  96.]\n",
      "[-12. -60.  32.  96.]\n",
      "[ -4. -60.  32.  96.]\n",
      "[  4. -60.  32.  96.]\n",
      "[ 12. -60.  32.  96.]\n",
      "[ 20. -60.  32.  96.]\n",
      "[ 28. -60.  32.  96.]\n",
      "[ 36. -60.  32.  96.]\n",
      "[ 44. -60.  32.  96.]\n",
      "[ 52. -60.  32.  96.]\n",
      "[ 60. -60.  32.  96.]\n",
      "[ 68. -60.  32.  96.]\n",
      "[-76. -52.  32.  96.]\n",
      "[-68. -52.  32.  96.]\n",
      "[-60. -52.  32.  96.]\n",
      "[-52. -52.  32.  96.]\n",
      "[-44. -52.  32.  96.]\n",
      "[-36. -52.  32.  96.]\n",
      "[-28. -52.  32.  96.]\n",
      "[-20. -52.  32.  96.]\n",
      "[-12. -52.  32.  96.]\n",
      "[ -4. -52.  32.  96.]\n",
      "[  4. -52.  32.  96.]\n",
      "[ 12. -52.  32.  96.]\n",
      "[ 20. -52.  32.  96.]\n",
      "[ 28. -52.  32.  96.]\n",
      "[ 36. -52.  32.  96.]\n",
      "[ 44. -52.  32.  96.]\n",
      "[ 52. -52.  32.  96.]\n",
      "[ 60. -52.  32.  96.]\n",
      "[ 68. -52.  32.  96.]\n",
      "[-76. -44.  32.  96.]\n",
      "[-68. -44.  32.  96.]\n",
      "[-60. -44.  32.  96.]\n",
      "[-52. -44.  32.  96.]\n",
      "[-44. -44.  32.  96.]\n",
      "[-36. -44.  32.  96.]\n",
      "[-28. -44.  32.  96.]\n",
      "[-20. -44.  32.  96.]\n",
      "[-12. -44.  32.  96.]\n",
      "[ -4. -44.  32.  96.]\n",
      "[  4. -44.  32.  96.]\n",
      "[ 12. -44.  32.  96.]\n",
      "[ 20. -44.  32.  96.]\n",
      "[ 28. -44.  32.  96.]\n",
      "[ 36. -44.  32.  96.]\n",
      "[ 44. -44.  32.  96.]\n",
      "[ 52. -44.  32.  96.]\n",
      "[ 60. -44.  32.  96.]\n",
      "[ 68. -44.  32.  96.]\n",
      "[-76. -36.  32.  96.]\n",
      "[-68. -36.  32.  96.]\n",
      "[-60. -36.  32.  96.]\n",
      "[-52. -36.  32.  96.]\n",
      "[-44. -36.  32.  96.]\n",
      "[-36. -36.  32.  96.]\n",
      "[-28. -36.  32.  96.]\n",
      "[-20. -36.  32.  96.]\n",
      "[-12. -36.  32.  96.]\n",
      "[ -4. -36.  32.  96.]\n",
      "[  4. -36.  32.  96.]\n",
      "[ 12. -36.  32.  96.]\n",
      "[ 20. -36.  32.  96.]\n",
      "[ 28. -36.  32.  96.]\n",
      "[ 36. -36.  32.  96.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 44. -36.  32.  96.]\n",
      "[ 52. -36.  32.  96.]\n",
      "[ 60. -36.  32.  96.]\n",
      "[ 68. -36.  32.  96.]\n",
      "[-76. -28.  32.  96.]\n",
      "[-68. -28.  32.  96.]\n",
      "[-60. -28.  32.  96.]\n",
      "[-52. -28.  32.  96.]\n",
      "[-44. -28.  32.  96.]\n",
      "[-36. -28.  32.  96.]\n",
      "[-28. -28.  32.  96.]\n",
      "[-20. -28.  32.  96.]\n",
      "[-12. -28.  32.  96.]\n",
      "[ -4. -28.  32.  96.]\n",
      "[  4. -28.  32.  96.]\n",
      "[ 12. -28.  32.  96.]\n",
      "[ 20. -28.  32.  96.]\n",
      "[ 28. -28.  32.  96.]\n",
      "[ 36. -28.  32.  96.]\n",
      "[ 44. -28.  32.  96.]\n",
      "[ 52. -28.  32.  96.]\n",
      "[ 60. -28.  32.  96.]\n",
      "[ 68. -28.  32.  96.]\n",
      "[-76. -20.  32.  96.]\n",
      "[-68. -20.  32.  96.]\n",
      "[-60. -20.  32.  96.]\n",
      "[-52. -20.  32.  96.]\n",
      "[-44. -20.  32.  96.]\n",
      "[-36. -20.  32.  96.]\n",
      "[-28. -20.  32.  96.]\n",
      "[-20. -20.  32.  96.]\n",
      "[-12. -20.  32.  96.]\n",
      "[ -4. -20.  32.  96.]\n",
      "[  4. -20.  32.  96.]\n",
      "[ 12. -20.  32.  96.]\n",
      "[ 20. -20.  32.  96.]\n",
      "[ 28. -20.  32.  96.]\n",
      "[ 36. -20.  32.  96.]\n",
      "[ 44. -20.  32.  96.]\n",
      "[ 52. -20.  32.  96.]\n",
      "[ 60. -20.  32.  96.]\n",
      "[ 68. -20.  32.  96.]\n",
      "[-76. -12.  32.  96.]\n",
      "[-68. -12.  32.  96.]\n",
      "[-60. -12.  32.  96.]\n",
      "[-52. -12.  32.  96.]\n",
      "[-44. -12.  32.  96.]\n",
      "[-36. -12.  32.  96.]\n",
      "[-28. -12.  32.  96.]\n",
      "[-20. -12.  32.  96.]\n",
      "[-12. -12.  32.  96.]\n",
      "[ -4. -12.  32.  96.]\n",
      "[  4. -12.  32.  96.]\n",
      "[ 12. -12.  32.  96.]\n",
      "[ 20. -12.  32.  96.]\n",
      "[ 28. -12.  32.  96.]\n",
      "[ 36. -12.  32.  96.]\n",
      "[ 44. -12.  32.  96.]\n",
      "[ 52. -12.  32.  96.]\n",
      "[ 60. -12.  32.  96.]\n",
      "[ 68. -12.  32.  96.]\n",
      "[-76.  -4.  32.  96.]\n",
      "[-68.  -4.  32.  96.]\n",
      "[-60.  -4.  32.  96.]\n",
      "[-52.  -4.  32.  96.]\n",
      "[-44.  -4.  32.  96.]\n",
      "[-36.  -4.  32.  96.]\n",
      "[-28.  -4.  32.  96.]\n",
      "[-20.  -4.  32.  96.]\n",
      "[-12.  -4.  32.  96.]\n",
      "[-4. -4. 32. 96.]\n",
      "[ 4. -4. 32. 96.]\n",
      "[12. -4. 32. 96.]\n",
      "[20. -4. 32. 96.]\n",
      "[28. -4. 32. 96.]\n",
      "[36. -4. 32. 96.]\n",
      "[44. -4. 32. 96.]\n",
      "[52. -4. 32. 96.]\n",
      "[60. -4. 32. 96.]\n",
      "[68. -4. 32. 96.]\n",
      "[-76.   4.  32.  96.]\n",
      "[-68.   4.  32.  96.]\n",
      "[-60.   4.  32.  96.]\n",
      "[-52.   4.  32.  96.]\n",
      "[-44.   4.  32.  96.]\n",
      "[-36.   4.  32.  96.]\n",
      "[-28.   4.  32.  96.]\n",
      "[-20.   4.  32.  96.]\n",
      "[-12.   4.  32.  96.]\n",
      "[-4.  4. 32. 96.]\n",
      "[ 4.  4. 32. 96.]\n",
      "[12.  4. 32. 96.]\n",
      "[20.  4. 32. 96.]\n",
      "[28.  4. 32. 96.]\n",
      "[36.  4. 32. 96.]\n",
      "[44.  4. 32. 96.]\n",
      "[52.  4. 32. 96.]\n",
      "[60.  4. 32. 96.]\n",
      "[68.  4. 32. 96.]\n",
      "[-76.  12.  32.  96.]\n",
      "[-68.  12.  32.  96.]\n",
      "[-60.  12.  32.  96.]\n",
      "[-52.  12.  32.  96.]\n",
      "[-44.  12.  32.  96.]\n",
      "[-36.  12.  32.  96.]\n",
      "[-28.  12.  32.  96.]\n",
      "[-20.  12.  32.  96.]\n",
      "[-12.  12.  32.  96.]\n",
      "[-4. 12. 32. 96.]\n",
      "[ 4. 12. 32. 96.]\n",
      "[12. 12. 32. 96.]\n",
      "[20. 12. 32. 96.]\n",
      "[28. 12. 32. 96.]\n",
      "[36. 12. 32. 96.]\n",
      "[44. 12. 32. 96.]\n",
      "[52. 12. 32. 96.]\n",
      "[60. 12. 32. 96.]\n",
      "[68. 12. 32. 96.]\n",
      "[-76.  20.  32.  96.]\n",
      "[-68.  20.  32.  96.]\n",
      "[-60.  20.  32.  96.]\n",
      "[-52.  20.  32.  96.]\n",
      "[-44.  20.  32.  96.]\n",
      "[-36.  20.  32.  96.]\n",
      "[-28.  20.  32.  96.]\n",
      "[-20.  20.  32.  96.]\n",
      "[-12.  20.  32.  96.]\n",
      "[-4. 20. 32. 96.]\n",
      "[ 4. 20. 32. 96.]\n",
      "[12. 20. 32. 96.]\n",
      "[20. 20. 32. 96.]\n",
      "[28. 20. 32. 96.]\n",
      "[36. 20. 32. 96.]\n",
      "[44. 20. 32. 96.]\n",
      "[52. 20. 32. 96.]\n",
      "[60. 20. 32. 96.]\n",
      "[68. 20. 32. 96.]\n",
      "[-76.  28.  32.  96.]\n",
      "[-68.  28.  32.  96.]\n",
      "[-60.  28.  32.  96.]\n",
      "[-52.  28.  32.  96.]\n",
      "[-44.  28.  32.  96.]\n",
      "[-36.  28.  32.  96.]\n",
      "[-28.  28.  32.  96.]\n",
      "[-20.  28.  32.  96.]\n",
      "[-12.  28.  32.  96.]\n",
      "[-4. 28. 32. 96.]\n",
      "[ 4. 28. 32. 96.]\n",
      "[12. 28. 32. 96.]\n",
      "[20. 28. 32. 96.]\n",
      "[28. 28. 32. 96.]\n",
      "[36. 28. 32. 96.]\n",
      "[44. 28. 32. 96.]\n",
      "[52. 28. 32. 96.]\n",
      "[60. 28. 32. 96.]\n",
      "[68. 28. 32. 96.]\n",
      "[-76.  36.  32.  96.]\n",
      "[-68.  36.  32.  96.]\n",
      "[-60.  36.  32.  96.]\n",
      "[-52.  36.  32.  96.]\n",
      "[-44.  36.  32.  96.]\n",
      "[-36.  36.  32.  96.]\n",
      "[-28.  36.  32.  96.]\n",
      "[-20.  36.  32.  96.]\n",
      "[-12.  36.  32.  96.]\n",
      "[-4. 36. 32. 96.]\n",
      "[ 4. 36. 32. 96.]\n",
      "[12. 36. 32. 96.]\n",
      "[20. 36. 32. 96.]\n",
      "[28. 36. 32. 96.]\n",
      "[36. 36. 32. 96.]\n",
      "[44. 36. 32. 96.]\n",
      "[52. 36. 32. 96.]\n",
      "[60. 36. 32. 96.]\n",
      "[68. 36. 32. 96.]\n",
      "[-76.  44.  32.  96.]\n",
      "[-68.  44.  32.  96.]\n",
      "[-60.  44.  32.  96.]\n",
      "[-52.  44.  32.  96.]\n",
      "[-44.  44.  32.  96.]\n",
      "[-36.  44.  32.  96.]\n",
      "[-28.  44.  32.  96.]\n",
      "[-20.  44.  32.  96.]\n",
      "[-12.  44.  32.  96.]\n",
      "[-4. 44. 32. 96.]\n",
      "[ 4. 44. 32. 96.]\n",
      "[12. 44. 32. 96.]\n",
      "[20. 44. 32. 96.]\n",
      "[28. 44. 32. 96.]\n",
      "[36. 44. 32. 96.]\n",
      "[44. 44. 32. 96.]\n",
      "[52. 44. 32. 96.]\n",
      "[60. 44. 32. 96.]\n",
      "[68. 44. 32. 96.]\n",
      "[-76.  52.  32.  96.]\n",
      "[-68.  52.  32.  96.]\n",
      "[-60.  52.  32.  96.]\n",
      "[-52.  52.  32.  96.]\n",
      "[-44.  52.  32.  96.]\n",
      "[-36.  52.  32.  96.]\n",
      "[-28.  52.  32.  96.]\n",
      "[-20.  52.  32.  96.]\n",
      "[-12.  52.  32.  96.]\n",
      "[-4. 52. 32. 96.]\n",
      "[ 4. 52. 32. 96.]\n",
      "[12. 52. 32. 96.]\n",
      "[20. 52. 32. 96.]\n",
      "[28. 52. 32. 96.]\n",
      "[36. 52. 32. 96.]\n",
      "[44. 52. 32. 96.]\n",
      "[52. 52. 32. 96.]\n",
      "[60. 52. 32. 96.]\n",
      "[68. 52. 32. 96.]\n",
      "[-76.  60.  32.  96.]\n",
      "[-68.  60.  32.  96.]\n",
      "[-60.  60.  32.  96.]\n",
      "[-52.  60.  32.  96.]\n",
      "[-44.  60.  32.  96.]\n",
      "[-36.  60.  32.  96.]\n",
      "[-28.  60.  32.  96.]\n",
      "[-20.  60.  32.  96.]\n",
      "[-12.  60.  32.  96.]\n",
      "[-4. 60. 32. 96.]\n",
      "[ 4. 60. 32. 96.]\n",
      "[12. 60. 32. 96.]\n",
      "[20. 60. 32. 96.]\n",
      "[28. 60. 32. 96.]\n",
      "[36. 60. 32. 96.]\n",
      "[44. 60. 32. 96.]\n",
      "[52. 60. 32. 96.]\n",
      "[60. 60. 32. 96.]\n",
      "[68. 60. 32. 96.]\n",
      "[-76.  68.  32.  96.]\n",
      "[-68.  68.  32.  96.]\n",
      "[-60.  68.  32.  96.]\n",
      "[-52.  68.  32.  96.]\n",
      "[-44.  68.  32.  96.]\n",
      "[-36.  68.  32.  96.]\n",
      "[-28.  68.  32.  96.]\n",
      "[-20.  68.  32.  96.]\n",
      "[-12.  68.  32.  96.]\n",
      "[-4. 68. 32. 96.]\n",
      "[ 4. 68. 32. 96.]\n",
      "[12. 68. 32. 96.]\n",
      "[20. 68. 32. 96.]\n",
      "[28. 68. 32. 96.]\n",
      "[36. 68. 32. 96.]\n",
      "[44. 68. 32. 96.]\n",
      "[52. 68. 32. 96.]\n",
      "[60. 68. 32. 96.]\n",
      "[68. 68. 32. 96.]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1805):\n",
    "    print(anchor[i,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb17e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69cfe81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_anchor(total_stride, scales, ratios, score_size):\n",
    "    anchor_num = len(ratios) * len(scales)\n",
    "    anchor = np.zeros((anchor_num, 4),  dtype=np.float32)\n",
    "    size = total_stride * total_stride\n",
    "    count = 0\n",
    "    for ratio in ratios:\n",
    "        # ws = int(np.sqrt(size * 1.0 / ratio))\n",
    "        ws = int(np.sqrt(size / ratio))\n",
    "        hs = int(ws * ratio)\n",
    "        for scale in scales:\n",
    "            wws = ws * scale\n",
    "            hhs = hs * scale\n",
    "            anchor[count, 0] = 0\n",
    "            anchor[count, 1] = 0\n",
    "            anchor[count, 2] = wws\n",
    "            anchor[count, 3] = hhs\n",
    "            count += 1\n",
    "\n",
    "    anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n",
    "    ori = - (score_size / 2) * total_stride\n",
    "    xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n",
    "                         [ori + total_stride * dy for dy in range(score_size)])\n",
    "    xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n",
    "             np.tile(yy.flatten(), (anchor_num, 1)).flatten()\n",
    "    anchor[:, 0], anchor[:, 1] = xx.astype(np.float32), yy.astype(np.float32)\n",
    "    return anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "baf062c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_anchor = generate_anchor(total_stride=8, scales= [8, ], \n",
    "                         ratios = [0.33, 0.5, 1, 2, 3], score_size =int(score_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0e63fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3ba1ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_size = (271-127) / 8 + 1\n",
    "_nchor= generate_anchor(total_stride=8, scales= [8, ], \n",
    "                         ratios = [0.33, 0.5, 1, 2, 3], score_size =int(score_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6da13844",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_stride=8 \n",
    "scales= [8, ]\n",
    "ratios = [0.33, 0.5, 1, 2, 3]\n",
    "score_size =int(score_size)\n",
    "\n",
    "anchor_num = len(ratios) * len(scales)\n",
    "anchor = np.zeros((anchor_num, 4),  dtype=np.float32)\n",
    "size = total_stride * total_stride\n",
    "count = 0\n",
    "for ratio in ratios:\n",
    "    # ws = int(np.sqrt(size * 1.0 / ratio))\n",
    "    ws = int(np.sqrt(size / ratio))\n",
    "    hs = int(ws * ratio)\n",
    "    for scale in scales:\n",
    "        wws = ws * scale\n",
    "        hhs = hs * scale\n",
    "        anchor[count, 0] = 0\n",
    "        anchor[count, 1] = 0\n",
    "        anchor[count, 2] = wws\n",
    "        anchor[count, 3] = hhs\n",
    "        count += 1\n",
    "\n",
    "anchor = np.tile(anchor, score_size * score_size).reshape((-1, 4))\n",
    "\n",
    "ori = - (score_size / 2) * total_stride\n",
    "\n",
    "xx, yy = np.meshgrid([ori + total_stride * dx for dx in range(score_size)],\n",
    "                    [ori + total_stride * dy for dy in range(score_size)])\n",
    "\n",
    "xx, yy = np.tile(xx.flatten(), (anchor_num, 1)).flatten(), \\\n",
    "         np.tile(yy.flatten(), (anchor_num, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f421d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-76., -76., 104.,  32.],\n",
       "       [-68., -76., 104.,  32.],\n",
       "       [-60., -76., 104.,  32.],\n",
       "       ...,\n",
       "       [ 52.,  68.,  32.,  96.],\n",
       "       [ 60.,  68.,  32.,  96.],\n",
       "       [ 68.,  68.,  32.,  96.]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1623275d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  4.,  4.,\n",
       "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "        4.,  4.,  4.,  4.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "        9.,  9.,  9.,  9.,  9.,  9.,  9.,  9., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "       11., 11., 11., 11., 11., 11., 11., 12., 12., 12., 12., 12., 12.,\n",
       "       12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "       13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13.,\n",
       "       13., 13., 13., 13., 13., 13., 14., 14., 14., 14., 14., 14., 14.,\n",
       "       14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 15.,\n",
       "       15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15.,\n",
       "       15., 15., 15., 15., 15., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "       16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 17., 17.,\n",
       "       17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "       17., 17., 17., 17., 18., 18., 18., 18., 18., 18., 18., 18., 18.,\n",
       "       18., 18., 18., 18., 18., 18., 18., 18., 18., 18.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(yy[:19*19]+76)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ed91b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61a658b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc034858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc71649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9763578",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e00dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac2e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a39ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282b9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b7b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668680e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d468434b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c0734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9184a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db454393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
